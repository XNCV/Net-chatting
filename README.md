# Project-Net-chatting-in-Computer-Networking-Course
The source codes in this project were referenced from many different sources. To respect the authors, I included the links that have the information used to complete this project. Finally, I describe the whole contributes of me and my partners.
# Chương 9: Các mạng tích chập *Mạng tích chập* (convolutional network), hay *mạng neuron tích chập* (convolutional neural network), hay CNN, là một loại mạng neuron đặc biệt để xử lý dữ liệu có cấu trúc dạng lưới. Chẳng hạn, dữ liệu dạng chuỗi thời gian (time-series) có thể được xem là lưới $1$ chiều chứa các mẫu được lấy tại các khoảng thời gian nhất định, hay dữ liệu hình ảnh có thể được xem là lưới $2$ chiều tạo bởi các điểm ảnh. Mạng tích chập đã và đang được áp dụng rất thành công trong nhiều ứng dụng thực tiễn. Tên gọi "mạng neuron tích chập" biểu thị việc mạng sử dụng phép toán *tích chập* (convolution). Tích chập là một dạng phép toán tuyến tính đặc biệt. *Mạng tích chập đơn giản là mạng neuron sử dụng phép tích chập thay vì phép nhân ma trận trong ít nhất một tầng của mạng*. Trong chương này, trước tiên, chúng tôi sẽ mô tả phép tích chập và giải thích lý do ta sử dụng nó trong mạng neuron. Sau đó, chúng tôi mô tả phép toán *gộp* (pooling); một phép toán được sử dụng trong hầu hết các mạng tích chập. Thông thường, phép tích chập sử dụng trong mạng neuron tích chập không hoàn toàn tương đồng với phép tích chập sử dụng trong các lĩnh vực khác, như trong toán thuần túy hay kỹ thuật. Chúng tôi sẽ trình bày một vài biến thể của hàm tích chập thường dùng trong các mạng neuron thực tế, cũng như cách áp dụng phép tích chập đối với các dạng dữ liệu khác nhau, với số chiều khác nhau. Tiếp đó, chúng tôi sẽ thảo luận về các phương pháp nâng cao hiệu suất của phép tích chập. Mạng tích chập là một ví dụ nổi bật về sự ảnh hưởng của các nguyên lý xuất phát từ ngành thần kinh học tới học sâu. Chúng tôi sẽ thảo luận về các nguyên lý thần kinh học này, sau đó đưa ra kết luận cùng với nhận xét về vai trò của mạng tích chập trong lịch sử phát triển của học sâu. Một chủ đề mà chương này sẽ không đề cập tới đó là làm thế nào để chọn cho mình một kiến trúc mạng tích chập phù hợp. Đúng vậy, mục tiêu của chương này chỉ là giải thích các công cụ mà mạng tích chập cung cấp, hướng dẫn tổng quát về cách chọn công cụ phù hợp trong các trường hợp cụ thể sẽ được trình bày ở chương 11. Làn sóng nghiên cứu về thiết kế các kiến trúc mạng tích chập đang phát triển nhanh đến mức mỗi vài tuần tới vài tháng lại có một kiến trúc mới tốt hơn được công bố cho một tác vụ nào đó, khiến chúng tôi không thể liệt kê ra kiến trúc nào là tốt nhất trong giới hạn của cuốn sách này. Tuy nhiên, các kiến trúc tốt nhất đó đã và đang sử dụng những thành tố nền tảng được chúng tôi mô tả dưới đây. # 9.1. Phép tích chập Định nghĩa chung nhất về phép tích chập là một phép toán giữa hai hàm có đối số thực. Chúng tôi sẽ đưa ra một vài ví dụ về hai hàm số kiểu này để giải thích rõ hơn về định nghĩa của phép tích chập. Giả sử ta đang theo dõi vị trí một con tàu vũ trụ bằng một cảm biến laser. Cảm biến laser này cung cấp một đầu ra duy nhất $x(t)$ là vị trí của con tàu tại thời điểm $t$. Trong đó cả $x$ và $t$ đều có giá trị thực, nghĩa là ta có thể nhận được kết quả của cảm biến laser tại bất kì thời điểm nào. Bây giờ, giả sử cảm biến laser bị nhiễu. Để giảm bớt sai lệch trong dự đoán vị trí con tàu, ta lấy trung bình có trọng số của một vài phép đo. Dĩ nhiên, các giá trị được đo càng gần thời điểm hiện tại càng quan trọng, nên ta sẽ gán cho chúng các trọng số lớn hơn. Ta có thể làm việc này bằng cách sử dụng một hàm gán trọng số $w(a)$, trong đó $a$ là "tuổi" của một phép đo. Nếu ta áp dụng phép lấy trung bình có trọng số như vậy tại mọi thời điểm, ta sẽ có một hàm số $s$ mới, là ước lượng đã được làm trơn của vị trí con tàu: $$\begin{eqnarray} s(t) = \int x(a)w(t-a)da.\tag{9.1} \end{eqnarray}$$ Phép toán này được gọi là *tích chập* (convolution), thường được ký hiệu bằng dấu sao ($*$). $$\begin{eqnarray} s(t) = (x * w)(t).\tag{9.2} \end{eqnarray}$$ Trong ví dụ trên, $w$ phải là một hàm mật độ xác suất hợp lệ, nếu không, ta sẽ không thu được đầu ra là trung bình có trọng số. Ngoài ra, trọng số mà $w$ gán cho tất cả các đối số âm phải bằng $0$, nếu không, đầu ra sẽ tính tới cả số liệu đo được trong tương lai, vốn nằm ngoài khả năng của chúng ta. Tuy nhiên, đó chỉ là các ràng buộc cụ thể dành riêng cho ví dụ này. Nói chung, phép tích chập là xác định với bất kỳ hai hàm số nào khi phép tích phân trên là xác định, và chúng có thể được sử dụng với mục đích khác chứ không chỉ là tính trung bình có trọng số. Trong hệ thống thuật ngữ của mạng tích chập, đối số đầu tiên của phép tích chập (trong ví dụ này là hàm $x$) thường được gọi là *đầu vào* (input), và đối số thứ hai (trong ví dụ này là hàm $w$) được gọi là *lõi* (kernel). Còn đầu ra đôi khi được gọi là *bản đồ đặc trưng* (feature map). Trong ví dụ của ta, việc cảm biến laser đo được vị trí con tàu ở mọi thời điểm bất kỳ là phi thực tế. Thông thường, khi làm việc với dữ liệu trên máy tính, thời gian sẽ bị rời rạc hóa, và cảm biến sẽ cung cấp dữ liệu cách nhau một khoảng thời gian nhất định. Do đó, để phù hợp với thực tế, ta giả định rằng cảm biến sẽ thực hiện đo đạc theo từng giây. Chỉ số thời gian $t$ lúc này chỉ có thể là số nguyên. Nếu giả định $x$ và $w$ được tính theo số nguyên $t$, chúng ta có thể định nghĩa phép tích chập rời rạc như sau: $$\begin{eqnarray} s(t) = (x * w)(t) = \sum^\infty_{a=-\infty}x(a)w(t-a).\tag{9.3} \end{eqnarray}$$ Trong các ứng dụng học máy, đầu vào thường là một mảng dữ liệu đa chiều, và lõi thường là một mảng đa chiều của các tham số thay đổi tuỳ theo các thuật toán học khác nhau. Ta sẽ gọi các mảng đa chiều này là các tensor. Do mỗi phần tử của đầu vào và lõi phải được lưu trữ hoàn toàn riêng biệt, ta thường giả định rằng các hàm này có giá trị $0$ tại mọi điểm, ngoại trừ tập hữu hạn một số điểm mà ta lưu trữ các giá trị. Nghĩa là, trong thực tế, chúng ta có thể tính tổng vô hạn ở công thức trên bằng cách tính tổng trên một lượng hữu hạn các phần tử của mảng. Cuối cùng, chúng ta thường sử dụng hàm tích chập trên nhiều hơn một trục toạ độ cùng lúc. Ví dụ, nếu dùng một bức ảnh hai chiều $I$ làm đầu vào, ta cần sử dụng một lõi hai chiều $K$: $$\begin{eqnarray} S(i,j) = (I*K)(i,j) = \sum_m \sum_n I(m,n)K(i-m,j-n).\tag{9.4} \end{eqnarray}$$ Phép tích chập có tính giao hoán, do đó ta có thể viết: $$\begin{eqnarray} S(i,j) = (K*I)(i,j) = \sum_m \sum_n I(i-m,j-n)K(m,n).\tag{9.5} \end{eqnarray}$$ Thường thì công thức thứ hai là đơn giản để cài đặt trong một thư viện học máy hơn, vì khoảng giá trị hợp lệ của $m$ và $n$ sẽ ít biến thiên hơn. Tính giao hoán của phép tích chập đến từ việc ta đã *đảo* (flip) lõi tương ứng với đầu vào, theo nghĩa là khi $m$ tăng, chỉ số gắn với đầu vào cũng tăng, nhưng chỉ số của lõi lại giảm. Lý do duy nhất cho việc thực hiện đảo lõi chỉ là để thu được tính giao hoán. Tính giao hoán trở hữu dụng trong việc chứng minh công thức, nhưng nó không quá quan trọng khi cài đặt mạng neuron. Thay vào đó, nhiều thư viện mạng neuron sử dụng một hàm số gọi là *tương quan chéo* (cross-correlation), tương tự với hàm tích chập nhưng không đảo lõi: $$\begin{eqnarray} S(i,j) = (K*I)(i,j) = \sum_m \sum_n I(i+m,j+n)K(m,n).\tag{9.6} \end{eqnarray}$$ Nhiều thư viện học máy thực hiện tương quan chéo nhưng gọi nó là tích chập. Trong cuốn sách này, chúng tôi sẽ theo quy ước này và gọi cả hai phép toán đều là tích chập, và sẽ nói rõ rằng thực sự có đảo lõi hay không ở những phần mà tính đảo lõi trở nên quan trọng. Trong các tác vụ học máy, thuật toán học sẽ học các giá trị phù hợp của lõi ở vị trí phù hợp. Do đó, một thuật toán học dựa trên phép tích chập có đảo lõi sẽ học ra một lõi đã được đảo, tương ứng với lõi học được bởi thuật toán không đảo lõi. Phép tích chập hiếm khi chỉ được dùng đơn lẻ trong học máy, mà thường được kết hợp đồng thời với các hàm khác, và sự kết hợp những hàm này là không giao hoán bất chấp phép tích chập có đảo lõi hay không. Hình 9.1 minh họa một ví dụ của phép tích chập (không đảo lõi) áp dụng cho một tensor $2$ chiều. ![](https://i.imgur.com/ACNxjIO.png) > Hình 9.1: Một ví dụ về tích chập $2$ chiều không đảo lõi. Chúng tôi chỉ biểu diễn kết quả tại các vị trí mà lõi nằm hoàn toàn trong ảnh, mà trong một số trường hợp còn được gọi là tích chập "hợp lệ". Chúng tôi vẽ các ô với mũi tên để mô tả cách tính phần tử ở góc trái trên của tensor đầu ra bằng cách áp dụng lõi vào vùng tương ứng của tensor đầu vào. Phép tích chập rời rạc có thể được coi như là phép nhân với một ma trận, tuy nhiên, ma trận này có ràng buộc là một số phần tử trong ma trận có giá trị bằng nhau. Ví dụ, với phép tích chập rời rạc đơn biến, mỗi hàng của ma trận bị ràng buộc rằng nó phải bằng hàng bên trên nó dịch đi $1$ phần tử. Đây gọi là *ma trận Toeplitz*. Đối với trường hợp hai chiều, phép tích chập tương ứng với *ma trận quay vòng theo khối kép* (doubly block circulant matrix). Ngoài những ràng buộc về sự bằng nhau của các phần tử trong ma trận, phép tích chập cũng thường tương ứng với một ma trận rất thưa (ma trận mà hầu hết các phần tử đều bằng $0$), bởi lõi thường nhỏ hơn nhiều so với ảnh đầu vào. Bất kì thuật toán mạng neuron nào thực thi phép nhân ma trận mà không phụ thuộc vào các tính chất cụ thể nào của cấu trúc ma trận, thì cũng sẽ thực thi được với phép tích chập mà không cần bất kì thay đổi nào trong mạng. Các mạng neuron tích chập điển hình sử dụng thêm các kĩ thuật chuyên biệt hơn để xử lý đầu vào lớn một cách hiệu quả, tuy nhiên về mặt lý thuyết thì việc này là không bắt buộc. # 9.2 Động lực phát triển Phép tích chập mở ra cơ hội giúp cải thiện hệ thống học máy dựa trên ba ý tưởng quan trọng: *tương tác thưa* (sparse interactions), *dùng chung tham số* (parameter sharing) và *biểu diễn đẳng biến* (equivariant reprentations). Hơn nữa, phép tích chập còn cung cấp công cụ để làm việc với đầu vào có kích thước thay đổi. Chúng tôi sẽ lần lược mô tả từng ý tưởng. Các tầng của mạng neuron truyền thống sử dụng phép nhân ma trận với một ma trận các tham số, trong đó mỗi tham số riêng biệt thể hiện tương tác giữa từng đơn vị đầu vào và đơn vị đầu ra. Nghĩa là tất cả đơn vị đầu ra đều tương tác với tất cả đơn vị đầu vào. Tuy nhiên, các mạng tích chập lại thường có các tương tác thưa (hay còn được gọi là *kết nối thưa* (sparse connectivity) hay *trọng số thưa* (sparse weights)). Điều này được thực hiện bằng cách sử dụng lõi nhỏ hơn đầu vào. Ví dụ, khi xử lý một bức ảnh, bức ảnh đầu vào có thể có hàng ngàn hay hàng triệu điểm ảnh, nhưng ta vẫn có thể phát hiện một tập đặc trưng nhỏ và có ý nghĩa, chẳng hạn như các cạnh với lõi chỉ chiếm vài chục hoặc vài trăm điểm ảnh. Nghĩa là ta chỉ cần lưu trữ một lượng tham số ít hơn, giúp giảm nhu cầu về bộ nhớ cũng như tăng tính hiệu quả về mặt thống kê của mô hình. Đồng thời, việc tính toán đầu ra cũng cần ít phép toán hơn. Những cải thiện này thường khá rõ rệt. Chẳng hạn, nếu có $m$ đầu vào và $n$ đầu ra, phép nhân ma trận cần $m \times n$ tham số, và các thuật toán trong thực tế có thời gian chạy là $O(m \times n)$ (cho mỗi mẫu). Nếu chúng ta giới hạn được số lượng kết nối của từng đầu ra xuống còn $k$; phương pháp kết nối thưa chỉ cần $k \times n$ tham số và mất thời gian chạy là $O(k \times n)$. Trong nhiều ứng dụng thực tế, ta có thể thu được kết quả tốt trong nhiều tác vụ học máy khi lấy $k$ nhỏ hơn $m$ vài bậc. Hình 9.2 và hình 9.3 minh hoạ kết nối thưa. Trong mạng tích chập đa tầng, các đơn vị trong các tầng sâu hơn có thể tương tác *gián tiếp* với một tỉ lệ đầu vào lớn hơn, như thể hiện trong hình 9.4. Điều này cho phép mạng mô tả một cách hiệu quả các tương tác phức tạp giữa nhiều biến bằng cách xây dựng các tương tác này từ nhiều khối đơn giản mà mỗi khối chỉ diễn tả các tương tác thưa. ![](https://i.imgur.com/Huarsml.png) > Hình 9.2: Kết nối thưa, nhìn theo hướng từ dưới lên. Chúng tôi tô đậm một đơn vị đầu vào $x_3$ và các đơn vị đầu ra trong $\boldsymbol s$ chịu ảnh hưởng bởi đầu vào này. *(Hình trên)* Khi $\boldsymbol s$ được tính bởi phép tích chập với một lõi có độ rộng $3$, chỉ có ba đầu ra bị ảnh hưởng bởi $\boldsymbol x$. *(Hình dưới)* Khi $\boldsymbol s$ được tính bởi phép nhân ma trận, kết nối không còn là thưa, do đó toàn bộ đầu ra đều bị ảnh hưởng bởi $x_3$. ![](https://i.imgur.com/WyxmbVY.png) > Hình 9.3: Kết nối thưa, nhìn theo hướng từ trên xuống. Chúng tôi tô đậm một đơn vị đầu ra $s_3$ và các đơn vị đầu vào trong $\boldsymbol x$ gây ảnh hưởng đến nó. Các đơn vị này được gọi là *trường tiếp nhận* (receptive field) của $s_3$. *(Hình trên)* Khi $\boldsymbol s$ được tính bởi phép tích chập với một lõi có độ rộng $3$, chỉ có ba đầu vào ảnh hưởng đến $s_3$. *(Hình phía dưới)* Khi $s$ được tính bởi phép nhân ma trận, kết nối không còn là thưa, toàn bộ đầu vào đều ảnh hưởng đến $s_3$. ![](https://i.imgur.com/Tqng3jP.png) > Hình 9.4. Trong mạng tích chập, trường tiếp nhận của các đơn vị trong những tầng sâu hơn sẽ lớn hơn trường tiếp nhận của các đơn vị ở các tầng nông. Hiệu ứng này sẽ còn tăng lên nếu mạng bao gồm các đặc điểm kiến trúc như *tích chập theo sải* (strided convolution) như hình 9.12 hay phép gộp (phần 9.3). Điều này có nghĩa là mặc dù các kết nối *trực tiếp* trong mạng tích chập là rất thưa, nhưng các đơn vị trong các tầng sâu hơn vẫn có thể được *gián tiếp* kết nối với toàn bộ hoặc một phần lớn hình ảnh đầu vào. *Dùng chung tham số* ám chỉ việc sử dụng cùng bộ tham số cho nhiều hơn một hàm trong mô hình. Trong mạng neuron truyền thống, từng phần tử của ma trận trọng số được sử dụng một lần duy nhất khi tính toán đầu ra của một tầng. Nó được nhân với một phần tử của đầu vào và không bao giờ được sử dụng lại nữa. Một cách tương đương, ta có thể nói rằng dùng chung tham số nghĩa là ma trận có *trọng số bị trói buộc* (tied weights), bởi giá trị của trọng số cho một đầu vào bị trói buộc với trọng số ở nơi khác. Trong mạng neuron tích chập, mỗi thành phần của lõi được sử dụng ở tất cả các vị trí của đầu vào (có lẽ chỉ trừ một vài điểm ảnh ở biên, tuỳ thuộc vào cách thiết kế để giải quyết trường hợp ở biên). Dùng chung tham số trong phép tích chập có nghĩa là thay vì học một bộ tham số riêng biệt cho từng vị trí, ta chỉ học một bộ tham số duy nhất. Điều này không ảnh hưởng tới thời gian chạy của quá trình lan truyền thuận-vẫn là $O(k \times n)$-nhưng làm giảm đáng kể nhu cầu lưu trữ của mô hình xuống còn $k$ tham số. Nhắc lại rằng: $k$ thường nhỏ hơn $m$ vài bậc. Do $m$ và $n$ thường có độ lớn gần tương đương, $k$ trên thực tế thường không đáng kể so với $m \times n$. Phép tích chập nhờ đó trở nên hiệu quả hơn rất nhiều so với phép nhân ma trận về mặt yêu cầu bộ nhớ và hiệu quả thống kê. Hình 9.5 minh họa cách mạng neuron dùng chung tham số. ![](https://i.imgur.com/EVADjoY.png) > Hình 9.5. Dùng chung tham số. Mũi tên đen biểu thị các kết nối dùng chung một tham số cụ thể trong hai mô hình khác nhau. *(Hình trên)* Các mũi tên đen biểu thị việc sử dụng phần tử trung tâm của một lõi $3$ phần tử trong mô hình tích chập. *(Hình dưới)* Mũi tên đen duy nhất biểu thị việc sử dụng phần tử trung tâm của ma trận trọng số trong mô hình được kết nối đầy đủ. Mô hình này không dùng chung tham số, do đó tham số chỉ được dùng một lần. Để lấy ví dụ cho hai quy tắc trên trong thực tế, hình 9.6 minh hoạ việc sử dụng kết nối thưa và dùng chung tham số có thể cải thiện đáng kể hiệu quả của một hàm tuyến tính trong việc nhận diện cạnh biên của ảnh. ![](https://i.imgur.com/EYpBIhD.png) > Hình 9.6. Hiệu quả của việc nhận diện cạnh biên. Hình bên phải được tạo thành bằng các lấy từng điểm ảnh trong ảnh gốc và trừ đi giá trị của điểm ảnh lân cận bên trái. Nó cho thấy cường độ của tất cả các cạnh hướng dọc trong ảnh đầu vào, và có thể là một phép toán hữu ích để nhận dạng vật thể. Cả hai bức ảnh đều có chiều cao là $280$ điểm ảnh. Ảnh đầu vào rộng $320$ điểm ảnh, trong khi ảnh đầu ra rộng $319$ điểm ảnh. Phép chuyển đổi này có thể được mô tả bằng một lõi tích chập chứa $2$ phần tử, và cần $319 \times 280 \times 3 = 267.960$ phép toán dấu phẩy động (hai phép nhân và một phép cộng cho mỗi điểm ảnh đầu ra) để tính toán bằng phép tích chập. Để mô tả phép biến đổi tương tự bằng phép nhân ma trận, ta cần $320 \times 280 \times 319 \times 280$, nghĩa là khoảng hơn $8$ tỷ phần tử trong ma trận, cho thấy phép tích chập hiệu quả hơn khoảng $4$ tỷ lần trong việc biểu diễn phép biến đổi này. Thuật toán nhân ma trận trực tiếp cần thực hiện hơn $16$ tỷ phép toán dấu phẩy động, cho thấy phép tích chập hiệu quả hơn gần $60,000$ lần về mặt tính toán. Tất nhiên, hầu hết các phần tử trong ma trận có thể bằng $0$. Nếu chúng ta chỉ lưu trữ các phần tử khác $0$ của ma trận, phép nhân ma trận và phép tích chập có thể cần lượng phép toán dấu phẩy động tương đương nhau trong tính toán. Ma trận vẫn sẽ cần phải chứa $2 \times 319 \times 280 = 178.640$ phần tử. Tích chập là một phương pháp cực kì hiệu quả để mô tả một phép biến đổi áp dụng cùng một biến đổi tuyến tính lên một vùng nhỏ cục bộ qua toàn bộ đầu vào. Nguồn: Paula Goodfellow. Trong trường hợp của phép tích chập, có một dạng cụ thể của dùng chung tham số khiến cho một tầng có một tính chất gọi là *tính đẳng biến* (equavariance) đối với phép tịnh tiến. Một hàm được gọi là có tính đẳng biến khi đầu vào thay đổi, và đầu ra cũng thay đổi theo cùng một cách như vậy. Cụ thể, hàm $f(x)$ đẳng biến với hàm $g$ nếu $f(g(x)) = g(f(x))$. Đối với phép tích chập, nếu ta cho hàm $g$ là một hàm tịnh tiến đầu vào, hay nói cách khác là dịch đầu vào, thì hàm tích chập là đẳng biến với $g$. Ví dụ, gọi $I$ là hàm biểu thị độ sáng của ảnh ở các tọa độ nguyên. Cho $g$ là một hàm ánh xạ một hàm ảnh tới hàm ảnh khác, sao cho $I' = g(I)$ là hàm ảnh với $I'(x,y) = I(x-1,y)$. Hàm này sẽ dịch toàn bộ các điểm ảnh của $I$ sang phải một đơn vị. Nếu ta áp dụng phép biến đổi này vào $I$, sau đó áp dụng tích chập, kết quả nhận được sẽ tương tự như khi áp dụng tích chập cho $I'$, sau đó áp dụng phép biến đổi $g$ vào đầu ra. Điều này có nghĩa là khi xử lý dữ liệu dạng chuỗi thời gian, phép tích chập tạo ra một loại dòng thời gian, đánh dấu khi nào các đặc trưng khác nhau xuất hiện trong đầu vào. Nếu tại đầu vào ta dịch một sự kiện về thời điểm muộn hơn, thì biểu diễn giống hệt của sự kiện đó sẽ xuất hiện trong đầu ra, chỉ là muộn hơn mà thôi. Tương tự với hình ảnh, tích chập tạo ra một ánh xạ $2$ chiều cho biết thời điểm các đặc trưng cụ thể xuất hiện trong đầu vào. Nếu ta dịch chuyển một đối tượng trong đầu vào, biển diễn của nó cũng sẽ bị dịch chuyển một lượng tương đương trong đầu ra. Tính chất này hữu ích khi ta biết rằng một hàm nào đó của một lượng nhỏ các điểm ảnh lân cận sẽ hữu ích khi áp dụng cho nhiều vị trí ở đầu vào. Ví dụ, khi xử lý ảnh, việc xác định được cạnh trong tầng đầu tiên của mạng tích chập thường hữu ích. Những cạnh giống nhau đều ít nhiều xuất hiện khắp nơi trong ảnh, vì vậy, sẽ thực tế hơn khi ta sử dụng chung các tham số trên toàn bộ bức ảnh. Tuy nhiên trong một vài trường hợp, nên tránh việc dùng chung tham số trên toàn bộ ảnh như vậy. Chẳng hạn, khi xử lý một bức ảnh đã được cắt để chỉ tập trung vào khuôn mặt một người, ta sẽ muốn trích xuất các đặc trưng khác nhau ở các vị trí khác nhau. Chẳng hạn, phần mạng phụ trách xử lý phần trên của khuôn mặt cần tìm kiếm lông mày, trong khi phần mạng xử lý phần dưới của khuôn mặt cần tìm kiếm cằm của người đó. Phép tích chập không có tính đẳng biến với một số phép biến đổi khác, như phóng to, thu nhỏ hay phép xoay. Cần áp dụng các cơ chế khác để xử lý những phép biến đổi này. Điều cuối cùng chúng tôi muốn nói ở phần này, đó là có một số dạng dữ liệu không thể được xử lý bằng các mạng neuron xác định bởi phép nhân với một ma trận kích thước cố định. Nhưng phép tích chập cho phép ta xử lý một số dạng dữ liệu này. Điều này sẽ được thảo luận sâu hơn trong phần 9.7. # 9.3. Phép gộp Một tầng mạng tích chập điển hình bao gồm ba giai đoạn (xem hình 9.7). Trong giai đoạn đầu tiên, tầng mạng thực hiện song song một vài phép tích chập để tạo ra một tập các tín hiệu kích hoạt tuyến tính (linear activations). Trong giai đoạn thứ hai, từng tín hiệu kích hoạt tuyến tính đi qua một hàm kích hoạt phi tuyến, chẳng hạn như hàm kích hoạt tuyến tính hiệu chỉnh (ReLU). Giai đoạn này đôi khi được gọi là là *giai đoạn phát hiện* (detector stage). Trong giai đoạn thứ ba, ta sẽ dùng *hàm gộp* (pooling function) để điều chỉnh đổi đầu ra của tầng một lần nữa. Một hàm gộp sẽ thay thế đầu ra của mạng tại một vị trí xác định bằng một thống kê tổng hợp các đầu ra lân cận. Ví dụ, phép *gộp cực đại* (max pooling) (Zhouand Chellappa, 1988) cho ta đầu ra cực đại trong vùng lân cận hình chữ nhật. Các hàm hộp phổ biến khác có thể kể đến như hàm tính giá trị trung bình của một vùng lân cận chữ nhật, tính chuẩn $L^2$ của vùng lân cận hình chữ nhật, hay tính trung bình có trọng số dựa trên khoảng cách tới điểm ảnh trung tâm. ![](https://i.imgur.com/1XQt5XN.png) > Hình 9.7: Các thành phần của một tầng mạng neuron tích chập điển hình. Có hai bộ thuật ngữ thường được sử dụng để mô tả các tầng này. *(Hình trái)* Trong bộ thuật ngữ này, mạng tích chập được xem như một một số lượng nhỏ các tầng tương đối phức tạp, với mỗi tầng có nhiều “giai đoạn”. Có một ánh xạ một-một giữa các tensor lõi với các tầng mạng. Trong cuốn sách này, chúng tôi thường sử dụng bộ thuật ngữ này. *(Hình phải)* Ở đây, mạng tích chập được xem như một số lượng lớn các tầng đơn giản; mỗi bước xử lý được coi là một tầng theo đúng nghĩa của nó. Điều này có nghĩa là không phải mọi “tầng” đều có tham số. Trong mọi trường hợp, phép toán gộp khiến cho biểu diễn gần như *bất biến* (invariant) đối với các phép tịnh tiến nhỏ của đầu vào. Tính bất biến đối với phép tịnh tiến có nghĩa là nếu ta dịch đầu vào một lượng nhỏ, giá trị của hầu hết đầu ra sau khi gộp sẽ không thay đổi. Hình 9.8 trình bày cách thức hoạt động của phép gộp. *Tính bất biến đối với phép tịnh tiến cục bộ có thể là một thuộc tính hữu ích khi ta quan tâm đến sự tồn tại của đặc trưng hơn là vị trí chính xác của nó*. Ví dụ, khi xác định một hình ảnh có chứa một khuôn mặt, ta không cần biết vị trí của mắt với độ chính xác đến từng điểm ảnh, mà chỉ cần biết rằng có một mắt ở phía bên trái của khuôn mặt và một mắt ở bên phải khuôn mặt là được. Nhưng trong một số tình huống khác, việc bảo toàn vị trí của các đặc trưng sẽ quan trọng hơn. Chẳng hạn, nếu muốn tìm một góc được xác định bởi hai cạnh cắt nhau tại một hướng cụ thể nào đó, cần phải bảo toàn đủ tốt vị trí của các cạnh để kiểm tra xem liệu chúng có cắt nhau hay không. ![](https://i.imgur.com/LU7rgcR.png) > Hình 9.8: Phép gộp cực đại tạo ra tính bất biến. *(Hình trên)* Một phần đầu ra của một tầng tích chập. Hàng dưới biểu diễn kết quả đầu ra của hàm phi tuyến. Hàng trên biểu diễn các kết quả đầu ra của phép gộp cực đại, với sải chập (stride) có độ dài là $1$ điểm ảnh giữa các vùng gộp và độ rộng vùng gộp là $3$ điểm ảnh. *(Hình dưới)* Vẫn trong trường hợp như hình trên, nhưng đầu vào bị dịch sang phải $1$ điểm ảnh. Mọi giá trị trong hàng dưới đã thay đổi, nhưng chỉ một nửa giá trị trong hàng trên bị thay đổi, bởi vì các bộ gộp cực đại chỉ nhạy với giá trị cực đại trong vùng lân cận, chứ không phải với vị trí chính xác của nó. Việc sử dụng phép gộp có thể được xem như là ta sẽ thêm vào một tiên nghiệm cực mạnh rằng: hàm mà các tầng học được phải là bất biến đối với các bước tịnh tiến nhỏ. Nếu giả định này chính xác, nó có thể cải thiện đáng kể hiệu quả thống kê của mạng. Phép gộp trên các miền không gian tạo ra tính bất biến đối với phép tịnh tiến, còn nếu chúng ta gộp đầu ra của các tích chập được tham số hoá riêng biệt, thì các đặc trưng có thể học được việc chúng sẽ trở nên bất biến đối với những phép biến đổi nào (xem hình 9.9). ![](https://i.imgur.com/QDpzfPl.png) > Hình 9.9: Ví dụ về tính bất biến tự học được. Một đơn vị gộp thực hiện phép gộp trên nhiều đặc trưng học được bằng các tham số riêng biệt có thể học để trở nên bất biến với các phép biến đổi của đầu vào. Ở đây, chúng tôi chỉ ra cách một tập gồm ba bộ lọc học được (learned filters) và một đơn vị gộp cực đại có thể học để trở nên bất biến đối với phép xoay. Ta mong đợi cả ba bộ lọc sẽ phát hiện được số $5$ viết tay. Mỗi bộ lọc sẽ cố gắng khớp với một hướng hơi khác nhau của số $5$. Khi số $5$ xuất hiện trong đầu vào, bộ lọc tương ứng sẽ khớp với nó và sinh ra một tín hiệu kích hoạt lớn trong đơn vị phát hiện (detector unit). Đơn vị gộp cực đại sau đó có một tín hiệu kích hoạt mạnh, bất kể đầu vào khớp với đơn vị phát hiện nào. Ở đây, chúng tôi chỉ ra cách quá trình mạng xử lý hai đầu vào khác nhau, kết quả là hai đơn vị phát hiện khác nhau đã được kích hoạt. Hiệu quả trên các đơn vị gộp là gần như nhau. Nguyên tắc này được tận dụng bởi các mạng cực đại đầu ra (Goodfellow et al., 2013a) và các mạng tích chập khác. Phép gộp cực đại trên các vị trí không gian là bất biến một cách tự nhiên đối với phép tịnh tiến; cách tiếp cận đa kênh này chỉ cần thiết cho việc học các phép biến đổi khác. Bởi vì phép gộp tổng hợp các phản hồi của toàn bộ vùng lân cận, ta có thể sử dụng số lượng đơn vị gộp ít hơn so với số lượng đơn vị phát hiện, bằng cách báo cáo các thống kê tổng hợp cho các vùng gộp có độ rộng $k$ điểm ảnh thay vì chỉ $1$ điểm ảnh. Như một ví dụ được minh hoạ ở hình 9.10. Điều này cải thiện hiệu suất tính toán của mạng vì tầng tiếp theo sẽ có số đầu vào cần xử lý ít hơn khoảng $k$ lần. Khi số lượng tham số trong tầng tiếp theo là một hàm của kích thước đầu vào (chẳng hạn như khi tầng tiếp theo là tầng kết nối đầy đủ và dựa trên phép nhân ma trận), việc giảm kích thước đầu vào này cũng có thể dẫn đến sự cải thiện hiệu suất thống kê và giảm lượng bộ nhớ yêu cầu dùng để lưu trữ các tham số. ![](https://i.imgur.com/QakMdJp.png) > Hình 9.10: Phép gộp với phép thu nhỏ mẫu (downsampling). Ở đây chúng ta sử dụng hàm gộp cực đại với độ rộng là $3$ và sải chập giữa các cụm gộp là $2$. Điều này làm giảm kích thước của biểu diễn xuống $2$ lần, giúp giảm cả gánh nặng tính toán lẫn gánh nặng thống kê ở tầng kế tiếp. Lưu ý rằng vùng gộp ngoài cùng bên phải có kích thước nhỏ hơn nhưng cần được tính vào nếu ta không muốn bỏ qua một số đơn vị phát hiện. Đối với nhiều tác vụ, phép gộp là rất cần thiết để xử lý các đầu vào có kích thước khác nhau. Ví dụ, khi muốn phân loại các hình ảnh có kích thước khác nhau, đầu vào cho tầng phân loại phải có kích thước cố định. Ta thường làm điều này bằng cách điều chỉnh độ lệch (offset) giữa các vùng gộp sao cho tầng phân loại luôn nhận được cùng một số lượng thống kê tổng hợp bất kể kích thước đầu vào là bao nhiêu. Ví dụ, tầng gộp cuối cùng của mạng có thể được xác định để xuất ra bốn bộ thống kê tổng hợp cho mỗi góc phần tư của hình, bất kể kích thước của bức ảnh. Một số công trình nghiên cứu lý thuyết cung cấp cho chúng ta những hướng dẫn về việc nên sử dụng dạng gộp nào trong các tình huống khác nhau [Boureau et al., 2010]. Ta cũng có thể gộp các đặc trưng với nhau một cách linh động, ví dụ, bằng cách chạy một thuật toán phân cụm trên các vị trí của các đặc trưng ta quan tâm [Boureau et al., 2011]. Cách tiếp cận này tạo ra các vùng gộp khác nhau cho mỗi hình ảnh. Một cách tiếp cận khác là *học* một cấu trúc gộp duy nhất, sau đó đem áp dụng cho tất cả các ảnh [Jia et al., 2012]. Phép gộp có thể làm phức tạp hoá một số loại kiến trúc mạng neuron sử dụng thông tin theo dạng từ trên xuống (top-down), chẳng hạn như máy Boltzmann và các bộ tự mã hóa. Những vấn đề này sẽ được thảo luận sâu hơn khi chúng tôi trình bày các loại mạng này trong phần III. Phép gộp trong các *máy Boltzmann tích chập* (convolutional Boltzmann machine) được trình bày trong phần 20.6. Các phép toán kiểu nghịch đảo trên các đơn vị gộp sẽ cần thiết trong một số mạng khả vi sẽ được nhắc tới trong phần 20.10.6. Hình 9.11 đưa ra một số ví dụ về các kiến trúc mạng tích chập hoàn chỉnh cho bài toán phân loại và phép gộp. ![](https://i.imgur.com/ByGtS6j.png) > Hình 9.11: Ví dụ về các kiến trúc cho bài toán phân loại với mạng tích chập. Độ lớn sải chập và độ sâu được sử dụng trong hình này không được khuyến nghị sử dụng trong thực tế; kiến trúc của chúng được thiết kế rất nông để phù hợp với khuôn khổ trang sách. Các mạng tích chập thực tiễn thường bao gồm số lượng nhánh nhiều đáng kể, không giống như các cấu trúc chuỗi đơn giản được biểu diễn ở đây. *(Hình trái)* Một mạng tích chập xử lý hình ảnh có kích thước cố định. Sau khi đi qua vài tầng tích chập và gộp xen kẽ nhau, tensor của bản đồ đặc trưng tích chập được định hình lại để làm phẳng các chiều không gian. Phần còn lại của mạng là một bộ phân loại sử dụng mạng lan truyền thuận thông thường, như đã mô tả trong chương 6. *(Hình giữa)* Một mạng tích chập xử lý các hình ảnh có kích thước khác nhau nhưng vẫn duy trì một thành phần kết nối đầy đủ. Mạng này sử dụng các phép toán gộp có kích thước khác nhau, nhưng số lượng phép gộp là cố định để cho ra một vector có kích thước cố định là $576$ đơn vị làm đầu vào cho phần kết nối đầy đủ của mạng. *(Hình phải)* Một mạng tích chập không có bất kỳ tầng kết nối trọng số đầy đủ nào. Thay vào đó, tầng tích chập cuối cùng xuất ra một bản đồ đặc trưng cho từng lớp phân loại. Mô hình này có thể học một phép ánh xạ về khả năng xảy ra của từng lớp tại mỗi vị trí không gian. Phép lấy trung bình một bản đồ đặc trưng để tạo ra một giá trị duy nhất làm đối số cho tầng phân loại cực đại mềm (softmax) ở trên cùng. # 9.4 Tích chập và phép gộp dưới góc nhìn một tiên nghiệm mạnh vô hạn Nhớ lại khái niệm về *phân phối xác suất tiên nghiệm* từ phần 5.6. Đây là một phân phối xác suất của các tham số của một mô hình giúp mã hoá niềm tin của chúng ta về câu hỏi những mô hình nào là hợp lý, trước khi quan sát bất kỳ dữ liệu nào. Các tiên nghiệm có thể được coi là yếu hoặc mạnh tùy thuộc vào mức độ tập trung của mật độ xác suất trong tiên nghiệm. Một tiên nghiệm yếu là một phân phối tiên nghiệm với entropy cao, chẳng hạn như một phân bố Gauss với phương sai lớn. Tiên nghiệm như vậy cho phép dữ liệu thay đổi các tham số một cách gần như tự do. Một tiên nghiệm mạnh có entropy rất thấp, chẳng hạn như một phân phối Gauss với phương sai thấp. Tiên nghiệm như vậy đóng vai trò tích cực hơn trong việc xác định miền mà giá trị mà các tham số rơi vào. Một tiên nghiệm mạnh vô hạn gán xác suất bằng $0$ cho một vài tham số và biểu thị rằng các giá trị tham số này hoàn toàn bị cấm, bất kể dữ liệu ủng hộ các giá trị đó như thế nào. Chúng ta có thể tưởng tượng mạng tích chập cũng tương tự như một mạng kết nối đầy đủ, với một tiên nghiệm mạnh vô hạn cho các trọng số của nó. Tiên nghiệm mạnh vô hạn này cho biết các trọng số cho một đơn vị ẩn sẽ phải đồng nhất với các trọng số của các đơn vị lân cận nhưng bị dịch đi trong không gian. Tiên nghiệm cũng cho biết các trọng số phải bằng $0$, trừ các giá trị tại các trường tiếp nhận nhỏ và tiếp giáp nhau được chỉ định cho đơn vị ẩn đó. Nhìn chung, ta có thể coi việc sử dụng tích chập như việc đưa ra một phân phối xác suất tiên nghiệm mạnh vô hạn cho các tham số trong một tầng. Tiên nghiệm này cho biết: hàm số mà mỗi tầng nên học chỉ chứa các tương tác cục bộ và có tính đẳng biến đối với các phép tịnh tiến. Tương tự như vậy, công dụng của phép gộp là một tiên nghiệm mạnh vô hạn mà theo đó, mỗi đơn vị nên có tính bất biến với các tịnh tiến nhỏ. Dĩ nhiên, việc triển khai mạng tích chập như một mạng kết nối đầy đủ với các tiên nghiệm mạnh vô hạn sẽ là rất lãng phí tài nguyên tính toán. Nhưng quan điểm coi mạng tích chập như một mạng kết nối đầy đủ với các tiên nghiệm mạnh vô hạn có thể giúp chúng ta hiểu rõ hơn về cách hoạt động của mạng tích chập. Một chi tiết quan trọng đó là phép tích chập và phép gộp có thể gây ra hiện tượng *vị khớp* (underfitting) cho mô hình. Giống như bất kỳ tiên nghiệm nào, phép chập và phép gộp chỉ hữu ích khi các giả định bởi tiên nghiệm là khá chính xác. Nếu một tác vụ yêu cầu việc bảo toàn chính xác thông tin về không gian, thì việc sử dụng phép gộp trên toàn bộ các đặc trưng sẽ làm tăng sai số huấn luyện. Một vài kiến trúc mạng tích chập [Szegedy et al. 2014a] được thiết kế để chỉ sử dụng phép gộp trên một số kênh, để có thể thu được cả các đặc trưng có tính bất biến cao lẫn các đặc trưng sẽ không bị vị khớp khi tiên nghiệm về bất biến tịnh tiến là không chính xác. Trong trường hợp bài toán yêu cầu kết hợp thông tin giữa các vị trí ở khoảng cách rất xa nhau ở đầu vào, thì tiên nghiệm áp đặt bởi phép tích chập có thể là không phù hợp. Một chi tiết quan trọng khác đó là chúng ta chỉ nên so sánh các mô hình mạng tích chập với nhau dựa trên các thang đo tiêu chuẩn về hiệu suất học thống kê. Các mô hình không sử dụng phép tích chập vẫn có thể học được ngay cả khi ta hoán đổi vị trí các điểm ảnh trong ảnh. Đối với nhiều bộ dữ liệu ảnh, có nhiều thang đo riêng biệt cho các mô hình là *bất biến theo hoán vị* (permutation invariant), và phải tự tìm ra những khái niệm về topology thông qua quá trình học và cho các mô hình có tri thức về các mối liên hệ không gian được gắn cứng bởi người thiết kế chúng. # 9.5 Các biến thể của hàm tích chập cơ bản Khi thảo luận về phép tích chập trong phạm vi mạng neuron, ta thường không đề cập một cách chính xác về các phép tích chập rời rạc chuẩn thường được sử dụng trong cộng đồng toán. Các hàm mà ta sử dụng trong những bài toán thực tế sẽ khác một chút. Trong phần này, chúng tôi sẽ mô tả chi tiết về những điểm khác nhau này và nhấn mạnh một số đặc tính quan trọng của các hàm được sử dụng trong mạng neuron. Trước tiên, khi đề cập đến phép tích chập trong phạm vi mạng neuron, nó thường mang nghĩa là một phép toán bao gồm nhiều phép tích chập song song. Bởi vì phép tích chập với một lõi duy nhất chỉ có thể trích xuất được một dạng đặc trưng, mặc dù là tại nhiều vị trí không gian khác nhau. Tuy nhiên ta thường mong muốn mỗi tầng trong mô hình mạng có thể trích xuất được nhiều dạng đặc trưng, tại nhiều vị trí khác nhau. Thêm vào đó, dữ liệu đầu vào thông thường không chỉ là một lưới các giá trị số thực mà sẽ là một lưới các quan sát có giá trị vector. Ví dụ, một ảnh màu sẽ bao gồm $3$ giá trị chỉ cường độ sắc đỏ, lục, lam tại mỗi điểm ảnh. Trong mô hình mạng neuron tích chập đa tầng, đầu vào của tầng thứ hai chính là đầu ra của tầng thứ nhất, chúng thường là kết quả của nhiều phép tích chập khác nhau tại mỗi vị trí. Khi làm việc với ảnh, chúng ta thường coi dữ liệu đầu vào và đầu ra của mạng tích chập như các tensor $3$ chiều với một chiều tương ứng với các kênh màu và hai chiều còn lại tương ứng với các tọa độ không gian của từng kênh màu. Các phần mềm hoạt động ở chế độ lô dữ liệu (batch mode), do vậy mô hình của ta thực tế sẽ sử dụng các tensor $4$ chiều, với chiều thứ tư giúp đánh chỉ mục cho các mẫu khác nhau trong khối dữ liệu, tuy nhiên, ở đây, chúng tôi sẽ bỏ qua chiều dữ liệu này để đơn giản hóa vấn đề. Do mạng tích chập thường sử dụng phép tích chập đa kênh, các phép biến đổi tuyến tính mà chúng dựa vào sẽ không đảm bảo được tính giao hoán, ngay cả khi áp dụng đảo lõi. Các phép biến đổi đa kênh chỉ có thể có tính giao hoán nếu như mỗi phép biến đổi có số kênh ở đầu ra bằng với số kênh đầu vào. Giả sử ta có một lõi tensor 4 chiều $\boldsymbol{\mathsf{K}}$, với phần tử ${\mathsf{K}}\_{i,j,k,l}$ cho biết cường độ kết nối giữa một đơn vị tại kênh thứ $i$ của đầu ra với một đơn vị tại kênh $j$ của đầu vào, với độ lệch $k$ hàng và $l$ cột giữa đơn vị đầu ra và đơn vị đầu vào. Giả sử rằng đầu vào của chúng ta chứa dữ liệu $\boldsymbol{\mathsf{V}}$ đã được quan sát, với phần tử ${\mathsf{V}}_{i,j,k}$ biểu thị giá trị đơn vị đầu vào ở kênh thứ $i$, tại điểm nằm ở hàng thứ $j$ và cột thứ $k$. Giả sử đầu ra của ta là $\boldsymbol{\mathsf{Z}}$ với định dạng tương tự như $\boldsymbol{\mathsf{V}}$. Nếu $\boldsymbol{\mathsf{Z}}$ là một kết quả của phép tích chập giữa tensor lõi $\boldsymbol{\mathsf{K}}$ và dữ liệu $\boldsymbol{\mathsf{V}}$ khi ta không áp dụng đảo lõi $\boldsymbol{\mathsf{K}}$, thì: $$\begin{eqnarray} Z_{i,j,k}=\sum\limits_{l,m,n}V_{l, j+m-1,k+n-1}K_{i,l,m,n} \tag{9.7} \end{eqnarray}$$ trong đó, ta lấy tổng trên $l,m,n$ trên toàn bộ giá trị mà tại đó chỉ số của tensor hợp lệ. Theo cách ký hiệu của đại số tuyến tính, ta đánh chỉ số các mảng bắt đầu từ $1$. Do đó, ta phải có $-1$ trong công thức trên. Với các ngôn ngữ lập trình như C và Python thì chỉ số sẽ bắt đầu từ $0$, do vậy biểu diễn công thức trên bằng ngôn ngữ lập trình sẽ đơn giản hơn. Chúng ta có thể sẽ muốn bỏ qua một vài vị trí của lõi để giảm chi phí tính toán (đổi lại bằng việc giảm bớt chất lượng của việc trích xuất các đặc trưng). Ta có thể coi việc này giống như bước áp dụng thu nhỏ mẫu (downsampling) đầu ra của hàm tích chập đầy đủ. Nếu chỉ muốn lấy mẫu mỗi $s$ điểm ảnh theo mỗi chiều tại đầu ra, ta có thể định nghĩa một hàm tích chập $c$ đã thu nhỏ mẫu như sau: $$\begin{eqnarray} Z_{i,j,k} = c(\boldsymbol{\mathsf{K}}, \boldsymbol{\mathsf{V}}, s)_{\,i,j,k} = \sum\limits_{l,m,n} [V_{l, \,(j-1)\times s +m, \,(k-1)\times s +n}\,K_{i,l,m,n}] \tag{9.8} \end{eqnarray}$$ Chúng tôi coi $s$ là tham số đại diện cho *sải chập* (stride) của phép tích chập đã thu nhỏ mẫu này. Ta cũng có thể định nghĩa một giá trị sải chập riêng cho mỗi hướng dịch chuyển. Xem minh hoạ ở hình 9.12. ![](https://i.imgur.com/wWWzRBG.png) >Hình 9.12: Phép tích chập theo sải. Trong ví dụ này, sải chập có độ lớn bằng $2$. *(Hình trên)* Phép tích chập với độ lớn sải bằng $2$ cho mỗi lượt tích chập. *(Hình dưới)* Phép tích chập với sải lớn hơn $1$ điểm ảnh là tương đương về mặt toán học với phép tích chập với sải bằng $1$, rồi áp dụng phép thu nhỏ mẫu. Rõ ràng, hướng tiếp cận hai bước ở hình dưới sẽ lãng phí tài nguyên tính toán, bởi nó tính toán rất nhiều giá trị rồi loại bỏ ngay sau đó. Một đặc tính cốt yếu khi thực thi bất kỳ mô hình mạng tích chập nào đó là ta có thể đệm thêm các giá trị 0 để mở rộng dữ liệu đầu vào. Nếu không có đặc tính này, kích thước của dữ liệu đầu vào sau khi đi qua mỗi tầng sẽ bị co lại $1$ điểm ảnh so với độ rộng của lõi ở mỗi tầng. Kỹ thuật đệm thêm các giá trị $0$ (zero padding) cho phép ta kiểm soát độ rộng của lõi và kích thước đầu ra một cách độc lập. Nếu không có kỹ thuật này, chúng ta buộc phải lựa chọn giữa việc sử dụng lõi nhỏ hay chấp nhận việc phạm vi không gian của mạng sẽ bị co lại một cách nhanh chóng. Cả hai kịch bản này đều làm giảm năng lực biểu đạt của mô hình mạng một cách rõ rệt. Như thể hiện trong ví dụ ở hình 9.13. Ta sẽ xem xét ba trường hợp đặc biệt trong thiết lập đệm thêm $0$. Đầu tiên là trường hợp khi ta hoàn toàn không sử dụng đệm thêm $0$, và lõi chỉ được phép đi qua những vị trí mà toàn bộ lõi nằm hoàn toàn trong ảnh. Thuật ngữ trong MATLAB gọi đây là phép tích chập *hợp lệ* (valid convolution). Trong trường hợp này, toàn bộ điểm ảnh tại đầu ra là một hàm của cùng một số lượng điểm ảnh trong đầu vào, do đó hành vi của điểm ảnh đầu ra có phần chuẩn tắc hơn. Tuy nhiên, kích thước đầu ra sẽ bị co lại dần qua mỗi tầng trong mô hình. Nếu ảnh đầu vào có độ rộng $m$ và lõi có độ rộng $k$, thì đầu ra sẽ có độ rộng là $m - k +1$. Tốc độ co kích thước này sẽ tăng lên nhanh chóng khi ta sử dụng lõi có kích thước lớn. Do mức co lại luôn lớn hơn $0$, nên số lượng tầng tích chập có thể đưa vào mô hình mạng sẽ bị giới hạn. Khi ta đưa thêm tầng vào mô hình, số chiều không gian của mạng sẽ giảm dần về $1 \times 1$, khi đó các tầng tích chập phía sau sẽ không còn ý nghĩa nữa. Trường hợp đặc biệt thứ hai trong thiết lập đệm thêm $0$ là ta sẽ thêm vừa đủ vùng đệm $0$ để kích thước đầu ra đúng bằng kích thước đầu vào. MATLAB gọi kỹ thuật này là tích chập *đều* (same convolution). Trong trường hợp này, mô hình mạng có thể thêm số lượng tầng tích chập tuỳ ý miễn là phù hợp với giới hạn tính toán của phần cứng, bởi phép tích chập thực hiện ở tầng trước không làm ảnh hưởng đến cấu trúc dữ liệu đầu vào ở tầng sau. Tuy nhiên, những điểm ảnh đầu vào gần vị trí biên của ảnh sẽ ảnh hưởng đến đầu ra ít hơn so với các điểm ảnh nằm ở trung tâm. Điều này sẽ dẫn đến việc các điểm ảnh nằm ở biên, theo một cách nào đó, sẽ không được thể hiện đầy đủ trong mô hình. Hạn chế này thúc đẩy sự phát triển của một trường hợp tiêu biểu thứ ba, mà MATLAB gọi là phép tích chập *đầy đủ* (full convolution), trong đó một số lượng vừa đủ giá trị $0$ được thêm vào để mỗi điểm ảnh có thể được sử dụng $k$ lần theo mỗi hướng, do đó ảnh đầu ra sẽ có kích thước là $m-k+1$. Trong trường hợp này, các điểm ảnh đầu ra ở vị trí gần biên là một hàm của ít điểm ảnh đầu vào hơn so với các điểm ảnh nằm ở trung tâm. Việc này có thể gây khó khăn cho việc học một lõi duy nhất sao cho nó hữu hiệu tại mọi vị trí trong bản đồ đặc trưng tích chập. Thông thường, số lượng vùng đệm $0$ tối ưu nhất (xét theo độ chính xác của tác vụ phân loại trên tập kiểm thử) nằm ở đâu đó giữa phép tích chập "hợp lệ" và phép tích chậm "đều". ![](https://i.imgur.com/1KSFf8K.png) > Hình 9.13: Ảnh hưởng của việc thêm vùng đệm $0$* lên kích thước của mô hình mạng. Xét một mạng tích chập với lõi có độ rộng bằng $6$ tại mọi tầng mạng. Trong ví dụ này, chúng tôi không sử dụng bất kỳ phép gộp nào, do đó chỉ có bản thân phép tích chập làm giảm kích thước của mô hình mạng. *(Hình trên)* Trong mạng tích chập này, ta không đệm thêm $0$, khiến ma trận biểu diễn bị co lại $5$ điểm ảnh qua mỗi tầng mạng. Dữ liệu đầu vào có $16$ điểm ảnh, ta chỉ có thể sử dụng $3$ tầng tích chập trong mô hình, hơn nữa tầng cuối cùng không thể dịch chuyển lõi được nên thực tế mô hình chỉ có thể áp dụng $2$ tầng tích chập mà thôi. Tốc độ co dữ liệu có thể được giảm bớt khi ta sử dụng lõi với kích cỡ nhỏ hơn, nhưng cũng đồng nghĩa với việc giảm khả năng biểu diễn, và việc bị co dữ liệu là không thể tránh khỏi trong kiểu kiến trúc này. *(Hình dưới)* Bằng cách ngầm thêm vào $5$ điểm ảnh có giá trị $0$ tại mỗi tầng, chúng ta ngăn ngừa được việc ma trận biểu diễn bị co lại. Điều này cho phép ta tạo ra một mạng tích chập với độ sâu tùy ý. Trong một số trường hợp, chúng ta không thực sự cần dùng đến tích chập, thay vào đó là các *tầng kết nối cục bộ* (locally connected layers) [LeCun, 1986, 198]. Trong trường hợp này, các ma trận kề trong đồ thị của mạng Perceptron đa tầng là giống nhau, tuy nhiên, mỗi kết nối đều có trọng số riêng được xác định bởi một tensor $6$ chiều: $\boldsymbol{\mathsf{W}}$. Các chỉ số của $\boldsymbol{\mathsf{W}}$ tương ứng như sau: $i$ là kênh đầu ra, $j$ là hàng đầu ra, $k$ là cột đầu ra, $l$ là kênh đầu vào, $m$ là độ lệch (offset) của hàng trong đầu vào, $n$ là độ lệch của cột trong đầu vào. Phần tuyến tính của tầng kết nối cục bộ có dạng sau: $$\mathsf{Z}_{i,j,k} = \sum\limits_{l,m,n} [\mathsf{V}_{l,\,j+m-1,\,k+n-1}\,w_{i,j,k,l,m,n}]. \tag{9.9}$$ Đôi khi ta gọi đây là *phép tích chập không dùng chung tham số* (unshared convolution), bởi nó giống như một phép tích chập rời rạc với lõi nhỏ, nhưng không dùng chung các tham số giữa các vị trí với nhau. Hình 9.14 so sánh giữa phương pháp kết nối cục bộ, phương pháp tích chập và phương pháp kết nối đầy đủ. ![](https://i.imgur.com/bw2Ke7k.png) > Hình 9.14: So sánh giữa kết nối cục bộ, tích chập và kết nối đầy đủ. *(Hình trên)* Một tầng kết nối cục bộ với kích thước mảnh (patch size) là $2$ điểm ảnh. Mỗi cạnh được gán một chữ cái độc nhất, thể hiện rằng các cạnh được liên kết với trọng số của riêng nó. *(Hình giữa)* Một tầng tích chập với lõi có chiều rộng là $2$ điểm ảnh. Mô hình này có cấu trúc kết nối giống hệt với tầng kết nối cục bộ. Điểm khác biệt không nằm ở cách tương tác giữa các đơn vị với nhau, mà là ở cách dùng chung tham số. Tầng kết nối cục bộ phía trên không dùng chung tham số. Tầng tích chập sử dụng cùng một cặp trọng số lặp đi lặp lại qua toàn bộ đầu vào, biểu diễn bằng các chữ cái lặp lại bên cạnh các mũi tên. *(Hình dưới)* Một tầng kết nối đầy đủ giống với tầng kết nối cục bộ ở chỗ mỗi cạnh đều có tham số của riêng nó (ở đây số lượng tham số là quá lớn để thể hiện bằng chữ cái). Tuy nhiên các kết nối không bị giới hạn như trường hợp kết nối cục bộ. Phương pháp sử dụng các tầng kết nối cục bộ sẽ hữu ích khi ta biết mỗi đặc trưng là một hàm số của một phần nhỏ của không gian ảnh, nhưng không có căn cứ để cho rằng đặc trưng đó cũng sẽ xuất hiện trên toàn bộ không gian ảnh. Ví dụ, nếu ta muốn biết một tấm ảnh có phải chụp một khuôn mặt hay không, ta chỉ cần tìm kiếm phần miệng ở nửa dưới thay vì toàn bộ bức ảnh. Đôi khi ta muốn tạo ra các phiên bản tích chập hoặc các tầng kết nối cục bộ có tính kết nối bị giới hạn hơn, chẳng hạn, để ràng buộc mỗi kênh đầu ra $i$ trở thành một hàm số của chỉ một tập con của các kênh đầu vào $l$. Một phương pháp thông thường để làm việc này đó là đưa $m$ kênh đầu tiên tại đầu ra kết nối với chỉ $n$ kênh đầu tiên tại đầu vào, $m$ kênh thứ hai tại đầu ra kết nối với chỉ $n$ kênh thứ hai tại đầu vào, vân vân. Hình 9.15 minh hoạ một ví dụ như vậy. Việc mô hình hoá các tương tác giữa một số lượng nhỏ các kênh cho phép mô hình mạng có ít tham số hơn, giảm mức tiêu thụ bộ nhớ trong quá trình tính toán, tăng hiệu quả thống kê và giảm số lượng tính toán cần thiết để thực hiện thuật toán lan truyền thuận và ngược. Phương pháp này giúp đạt được các mục đích trên mà không cần phải giảm số lượng đơn vị ẩn. ![](https://i.imgur.com/dr3iOv5.png) >Hình 9.15: Một mạng tích chập với $2$ kênh đầu ra đầu tiên được kết nối với $2$ kênh đầu vào đầu tiên, và $2$ kênh đầu ra thứ hai được kết nối đến $2$ kênh đầu vào thứ hai. *Tích chập xếp chồng* (tiled convolution) [Gregor and LeCun, 2010a; Le et al., 2010] yêu cầu sự dung hoà giữa một tầng tích chập và một tầng kết nối cục bộ. Thay vì học riêng biệt từng bộ trọng số tại mỗi vị trí, ta sẽ học một bộ các lõi mà ta có thể xoay khi dịch chuyển qua không gian ảnh. Điều này có nghĩa là các vị trí kề nhau sẽ có các bộ lọc khác nhau giống như trong một tầng kết nối cục bộ, tuy nhiên, yêu cầu bộ nhớ để lưu trữ các tham số sẽ tăng lên theo tỉ lệ kích thước của tập lõi này, thay vì tăng theo kích thước của toàn bộ bản đồ đặc trưng đầu ra. Hình 9.16 thể hiện một so sánh giữa các tầng kết nối cục bộ, tích chập xếp chồng và tích chập tiêu chuẩn. ![](https://i.imgur.com/h6MxPOW.png) > Hình 9.16: So sánh giữa các tầng kết nối cục bộ, tích chập xếp chồng và tích chập tiêu chuẩn. Cả ba đều có cùng tập các kết nối giữa các đơn vị khi sử dụng lõi có cùng kích thước. Sơ đồ này mô phỏng việc sử dụng lõi có chiều rộng $2$ điểm ảnh. Sự khác biệt giữa các phương pháp này nằm ở cách chúng chia sẻ tham số. *(Hình trên)* Tầng kết nối cục bộ không dùng chung tham số. Ta thấy rằng mỗi kết nối có trọng số của riêng nó và mỗi liên kết được ký hiệu bằng một chữ cái riêng biệt. *(Hình giữa)* Tích chập xếp chồng có một bộ $t$ lõi khác nhau. Ở đây chúng tôi mô phỏng trường hợp $t=2$, một lõi có các liên kết được ký hiệu là "$a$" và "$b$", lõi còn lại có liên kết được ký hiệu là "$c$" và "$d$". Mỗi lần ta dịch đầu ra sang phải một điểm ảnh, thì ta cũng chuyển sang sử dụng lõi kế tiếp. Điều này có nghĩa là, giống như tầng kết nối cục bộ, các đơn vị liền kề tại đầu ra có các tham số khác nhau. Tuy nhiên, không giống như tầng kết nối cục bộ, sau khi ta đã sử dụng hết $t$ lõi, chúng ta sẽ quay vòng trở lại với lõi đầu tiên. Hai đơn vị đầu ra cách nhau một khoảng bằng $t$ bước sẽ dùng chung tham số với nhau. *(Hình dưới)* Tích chập thông thường, tương đương với tích chập xếp chồng có $t=1$. Nó chỉ áp dụng duy nhất một lõi tại mọi điểm trên ảnh, như ta thấy ở trên hình, lõi với các trọng số được gán nhãn "$a$" và "$b$" được sử dụng ở mọi vị trí. Để định nghĩa tích chập xếp chồng về mặt đại số, đặt $\boldsymbol{\mathsf{K}}$ là một tensor $6$ chiều, trong đó có $2$ chiều tương ứng với các vị trí khác nhau của dữ liệu đầu ra. Thay vì sử dụng một chỉ số riêng biệt cho mỗi vị trí trên ánh xạ đầu ra, các vị trí ở đầu ra sẽ xoay vòng trên một tập hợp $t$ lựa chọn khác nhau của ngăn xếp lõi (kernel stack) theo mỗi hướng. Nếu $t$ bằng với chiều rộng của đầu ra, tích chập xếp chồng sẽ giống như một tầng kết nối cục bộ: $$\begin{eqnarray} \mathsf{Z}_{i,j,k} = \sum\limits_{l,m,n} \mathsf{V}_{l,\,j+m-1,\,k+n-1}\mathsf{K}_{i,l,m,n, j\%t+1,\,k\%t +1},\tag{9.10} \end{eqnarray}$$ trong đó $\%$ là phép toán chia lấy dư, ví dụ $t\,\%\,t = 0$, $(t+1)\%t=1$, vân vân. Ta có thể dễ dàng tổng quát hóa phương trình này cho trường hợp kích thước ngăn xếp lõi khác nhau ở mỗi chiều. Các tầng kết nối cục bộ và các tầng tích chập xếp chồng đều có một sự tương tác thú vị với phép gộp cực đại: các đơn vị phát hiện của những tầng này được dẫn dắt bởi các bộ lọc khác nhau. Nếu những bộ lọc này được học để phát hiện các biến thể khác nhau của cùng một bộ đặc trưng, thì các đơn vị gộp cực đại sẽ trở nên bất biến đối với các phép biến đổi đã học được (xem hình 9.9). Các tầng tích chập được thiết kế để trở nên bất biến, cụ thể là với các phép tịnh tiến. Ngoài phép tích chập, các mạng tích chập cũng thường cần thực hiện một số phép toán khác. Để thực thi quá trình học từ dữ liệu, mô hình phải có khả năng tính toán gradient theo lõi, khi biết gradient theo đầu ra. Trong một số trường hợp đơn giản, phép toán này có thể được thực hiện bằng phép tích chập, nhưng trong nhiều trường hợp ta quan tâm, chẳng hạn như khi giá trị sải chập lớn hơn $1$, sẽ không có đặc tính này. Nhắc lại: tích chập là một phép toán tuyến tính và do đó có thể được mô tả dưới dạng một phép nhân ma trận (nếu trước đó ta định dạng lại tensor đầu vào thành một vector phẳng). Ma trận liên quan là một hàm số của lõi tích chập. Ma trận đó là ma trận thưa, và mỗi phần tử của lõi được sao chép sang một vài phần tử của ma trận đó. Hướng nhìn này giúp chúng ta suy ra một vài phép toán khác cần thiết trong quá trình thực thi mô hình mạng tích chập. Phép nhân với ma trận chuyển vị của ma trận xác định bởi phép tích chập là một trong những phép toán như vậy. Nó là phép toán cần thiết để lan truyền ngược các đạo hàm của hàm sai số qua một tầng tích chập, do đó ta cần sử dụng phép nhân này khi huấn luyện mạng tích chập có nhiều hơn một tầng ẩn. Phép toán này cũng cần trong việc tái thiết các đơn vị khả kiến (visible) từ những đơn vị ẩn (hidden) [Simard et al., 1992]. Tái thiết các đơn vị khả kiến là một phép toán được sử dụng phổ biến trong các mô hình ở phần III của quyển sách này, như bộ tự mã hóa, máy Boltzmann thu hẹp và mã hóa thưa. Ta cần sử dụng phép tích chập chuyển vị để xây dựng các phiên bản tích chập của những mô hình này. Giống như phép tính gradient theo lõi, phép tính gradient theo đầu vào này đôi khi có thể được thực hiện bằng một phép tích chập nhưng thường yêu cầu thực hiện thêm một phép toán thứ ba. Cần thận trọng khi phối hợp phép chuyển vị ma trận này với quá trình lan truyền thuận. Kích thước ma trận đầu ra sau khi chuyển vị phụ thuộc vào cách ta sử dụng kỹ thuật đệm thêm $0$ và độ lớn sải chập trong quá trình tính toán lan truyền thuận, cũng như kích thước của ánh xạ đầu ra sau quá trình lan truyền thuận. Trong một số trường hợp, nhiều đầu vào có kích thước khác nhau có thể trả về ánh xạ đầu ra của phép lan truyền thuận có cùng một kích thước, do vậy phép chuyển vị phải biết được chính xác kích thước của dữ liệu đầu vào ban đầu. Ba phép toán trên — tích chập, lan truyền ngược từ đầu ra về các trọng số và lan truyền ngược từ đầu ra về đầu vào — là đủ để tính toán mọi gradient cần thiết cho quá trình huấn luyện bất cứ mô hình mạng tích chập lan truyền thuận nào với độ sâu bất kỳ, cũng như quá trình huấn luyện các mô hình mạng tích chập với các hàm tái thiết dựa trên chuyển vị của phép tích chập. Tham khảo Goodfellow (2010) về một phương pháp đầy đủ để suy ra các phương trình trong trường hợp tổng quát đa chiều, đa mẫu huấn luyện. Để hiểu phần nào cách hoạt động của các phương trình này, chúng tôi sẽ trình bày phiên bản $2$ chiều với chỉ một mẫu huấn luyện. Giả sử ta cần huấn luyện một mạng tích chập với ngăn xếp lõi $\mathsf{K}$, áp dụng cho ảnh đa kênh $\mathsf{V}$ với bước sải $s$, được định nghĩa bởi $c(\mathsf{K}, \mathsf{V}, s)$ như trong phương trình 9.8. Giả sử ta muốn cực tiểu hoá một hàm mất mát $J(\mathsf{V}, \mathsf{K})$ nào đó. Trong quá trình lan truyền thuận, ta sẽ cần sử dụng chính $c$ để tính đầu ra $\mathsf{Z}$. $\mathsf{Z}$ sau đó được lan truyền xuyên suốt toàn bộ phần còn lại của mạng và được sử dụng để tính toán hàm chi phí $J$. Trong quá trình lan truyền ngược, ta sẽ nhận được một tensor $\mathsf{G}$ sao cho $\mathsf{G}_{ i , j , k } = \frac { \partial } { \partial \mathsf{Z} _ { i , j , k } } J (\mathsf{V} , \mathsf{K})$. Để huấn luyện mạng này ta cần tính các đạo hàm theo những trọng số trong lõi. Để làm được điều này, ta có thể sử dụng hàm $$\begin{eqnarray} g ( \mathsf { G } , \mathsf { V } , s ) _ { i , j , k , l } = \frac { \partial } { \partial \mathsf { K } _ { i , j , k , l } } J ( \mathsf { V } , \mathsf { K } ) = \sum _ { m , n } \mathsf { G } _ { i , m , n } \mathsf { V } _ { j ,\, ( m - 1 ) \times s + k ,\, ( n - 1 ) \times s + l } \tag{9.11} \end{eqnarray}$$ Nếu tầng này không phải tầng dưới cùng của mạng, ta cần tính gradient tương ứng đối với $\mathsf{V}$ để lan truyền ngược sai số xuống tầng dưới. Ta có thể sử dụng hàm: $$\begin{eqnarray} h(\mathsf{K}, \mathsf{G}, s)_{i,j,k} = \frac{\partial } {\partial\mathsf{V}_{i, j,k }} J(\mathsf{V}, \mathsf {K}) \tag{9.12} \\\\ = \sum_{l,m\,với\atop(l - 1) \times s + m = j }\quad \sum _ { n , p \, với \atop (n-1) \times s + p = k } \sum _ { q } \mathsf{K} _ { q , i , m , p } \mathsf{G} _ { q , l , n }.\tag{9.13} \end{eqnarray}$$ *Mạng tự mã hóa* (autoencoder network), được trình bày trong chương 14, là loại mạng lan truyền thuận được huấn luyện để sao chép đầu vào của mạng tới đầu ra của mạng. Một ví dụ đơn giản chính là thuật toán phân tích thành phần chính (PCA), nó sao chép đầu vào $\boldsymbol { x }$ thành một biểu diễn tái thiết xấp xỉ $\boldsymbol {r}$ bằng cách sử dụng hàm $\boldsymbol { W } ^ { \top } \boldsymbol { W } \boldsymbol { x }$. Các bộ tự mã hoá tổng quát thường sử dụng phép nhân với chuyển vị của ma trận trọng số giống như cách mà PCA làm. Để khiến cho các mô hình như vậy có tính tích chập, ta có thể sử dụng hàm $h$ để thực hiện phép chuyển vị cho phép toán tích chập. Giả sử ta có các đơn vị ẩn $\mathsf{H}$ có cùng định dạng với $\mathsf{Z}$, và ta định nghĩa một biểu diễn tái thiết $$\begin{eqnarray} \mathsf { R } = h ( \mathsf { K } , \mathsf { H } , s ) \tag{9.14} \end{eqnarray}$$ Để huấn luyện bộ tự mã hóa, ta sẽ nhận được gradient theo $\mathsf {R}$ có dạng một tensor $\mathsf {E}$. Để huấn luyện bộ giải mã, ta cần tính gradient theo $\mathsf {K}$. Gradient này giả sử là $g (\mathsf {H}, \mathsf {E}, s)$. Để huấn luyện bộ mã hoá, ta cần tính gradient theo $\mathsf {H}$. Gradient này giả sử là $c (\mathsf {K}, \mathsf {E}, s)$. Ta cũng có thể lấy vi phân qua $g$ bằng cách sử dụng $c$ và $h$, nhưng các toán tử này là không cần thiết cho thuật toán lan truyền ngược trên bất kỳ kiến trúc mạng tiểu chuẩn nào. Nhìn chung, chúng ta sẽ không chỉ sử dụng toán tử tuyến tính để chuyển đổi đầu vào thành đầu ra trong một tầng tích chập. Ta thường phải cộng thêm hệ số tự do cho mỗi đầu ra trước khi áp dụng toán tử phi tuyến. Điều này đặt ra câu hỏi về cách dùng chung tham số giữa các hệ số tự do. Đối với các tầng kết nối cục bộ, ta sẽ gán cho mỗi đơn vị một hệ số tự do của riêng nó. Đối với tích chập xếp chồng, theo lẽ tự nhiên, ta sẽ dùng chung các hệ số tự do và sử dụng kiến trúc xếp chồng tương tự các lõi. Đối với các tầng tích chập, thông thường sẽ có một hệ số tự do ứng với mỗi kênh của đầu ra và sẽ được dùng chung trên toàn bộ vị trí không gian trong mỗi ánh xạ tích chập. Tuy nhiên, nếu đầu vào có kích thước cố định biết trước, ta cũng có thể học hệ số tự do riêng biệt cho từng vị trí của ánh xạ đầu ra. Việc tách rời các hệ số tự do như vậy có thể làm giảm hiệu quả thống kê của mô hình một chút, nhưng nó cho phép mô hình hiệu chỉnh sự khác biệt trong các thống kê của ảnh ở các vị trí khác nhau. Ví dụ, khi sử dụng kĩ thuật đệm thêm $0$ ngầm định, các đơn vị phát hiện (detector unit) ở cạnh biên của ảnh nhận được tổng số đầu vào ít hơn và có thể cần các hệ số tự do lớn hơn. # 9.6 Các đầu ra có cấu trúc Các mạng tích chập có thể được sử dụng để xuất ra một đối tượng có cấu trúc với số chiều lớn, thay vì chỉ dự đoán một nhãn của lớp cho tác vụ phân loại hoặc một giá trị thực cho tác vụ hồi quy. Thông thường, đối tượng này chỉ là một tensor, được tạo ra bởi một tầng tích chập tiêu chuẩn. Ví dụ, mô hình này có thể tạo ra một tensor $\mathsf{S}$, trong đó $\mathsf{S} _ { i , j , k }$ là xác suất mà điểm ảnh đầu vào $(j, k)$ thuộc lớp $i$, cho phép mô hình gán nhãn mọi điểm ảnh trong một bức ảnh và vẽ ra các mặt nạ chính xác tuân theo phác hoạ của từng đối tượng riêng lẻ. Một vấn đề thường nảy sinh đó là kích thước đầu ra có thể nhỏ hơn kích thước đầu vào, như minh hoạ trong hình 9.13. Trong các loại kiến trúc thường dùng để phân loại chỉ một đối tượng duy nhất trong một bức ảnh, sự suy giảm lớn nhất về kích thước không gian của mạng là do việc sử dụng các tầng gộp với bước sải lớn. Để tạo ra một ánh xạ đầu ra có cùng kích thước với đầu vào, người ta có thể tránh dùng phép gộp [Jain et al., 2007], hay một chiến lược đơn giản khác là tạo ra một lưới các nhãn có độ phân giải thấp hơn [Pinheiro and Collobert, 2014, 2015]. Cuối cùng, về nguyên tắc, người ta có thể sử dụng một toán tử gộp với sải chập đơn vị (sải có độ lớn bước bằng $1$). Một chiến lược gán nhãn từng điểm ảnh trong ảnh là tạo ra dự đoán ban đầu cho các nhãn của ảnh, sau đó tinh chỉnh lại giá trị dự đoán này bằng cách sử dụng mối tương tác giữa các điểm ảnh lân cận nhau. Lặp lại bước tinh chỉnh này nhiều lần có ý nghĩa tương tự như việc sử dụng cùng một phép tích chập ở mỗi giai đoạn, với các trọng số dùng chung giữa các tầng cuối cùng của mạng [Jain et al., 2007]. Điều này khiến cho một chuỗi tính toán thực hiện bởi các tầng tích chập liên tiếp dùng chung trọng số với nhau giống như một dạng mạng truy hồi đặc biệt [Pinheiro and Collobert, 2014, 2015]. Hình 9.17 biểu diễn kiến trúc của một mạng tích chập truy hồi như vậy. ![](https://i.imgur.com/FJb1Ppj.png) > Hình 9.17: Một ví dụ về mạng tích chập truy hồi sử dụng cho mục đích gán nhãn điểm ảnh. Dữ liệu đầu vào là một tensor ảnh $\mathsf{X}$, với các trục toạ độ tương ứng với hàng, cột và kênh màu (đỏ, lục, lam) của ảnh. Mục đích của mạng là tạo ra một tensor các nhãn $\hat { Y }$, cùng với một phân phối xác suất của các nhãn cho mỗi điểm ảnh. Tensor này có trục toạ độ tương ứng với hàng, cột của ảnh và các lớp khác nhau. Thay vì xuất ra $\hat{ Y }$ sau chỉ một lượt, mạng truy hồi lặp lại việc tinh chỉnh ước lượng $\hat { Y }$ của nó bằng cách sử dụng ước lượng trước đó của $\hat { Y }$ làm đầu vào để tạo ra ước lượng mới. Các tham số giống nhau được dùng cho mỗi lần cập nhật ước lượng, và ước lượng đó có thể được tinh chỉnh bao nhiêu lần tùy thích. Tensor lõi $\mathsf{U}$ được sử dụng ở mỗi bước để tính toán các biểu diễn ẩn (hidden representation) trong ảnh đầu vào. Tensor lõi $\mathsf{V}$ được dùng để tạo ra một ước lượng cho các nhãn từ các giá trị ẩn đã biết. Tại tất cả các bước trừ bước đầu tiên, các lõi $\mathsf{W}$ được nhân chập với $\hat { Y }$ để trở thành đầu vào cho tầng ẩn. Ở bước đầu tiên, số hạng này được thay thế bằng giá trị $0$. Bởi vì mô hình sử dụng cùng một bộ tham số tại mỗi bước, đây là ví dụ cho một mạng truy hồi, sẽ được mô tả ở chương 10. Khi đã đưa ra dự đoán cho mỗi điểm ảnh, ta có thể sử dụng nhiều phương pháp khác nhau để tiếp tục xử lý các dự đoán này nhằm thu được một hình ảnh đã được phân thành các vùng [Briggman et al., 2009; Turaga et al., 2010; Farabet et al., 2013]. Ý tưởng chung của những phương pháp này là đưa ra một giả định rằng: các nhóm lớn của những điểm ảnh liên tiếp nhau thường có xu hướng được gán chung một nhãn. Các mô hình đồ thị có thể mô tả các mối quan hệ về mặt xác suất giữa các điểm ảnh lân cận. Ngoài ra, mạng tích chập có thể được huấn luyện để cực đại hoá một xấp xỉ của hàm mục tiêu của mô hình đồ thị trên [Ning et al., 2005; Thompson et al., 2014]. # 9.7 Các kiểu dữ liệu Dữ liệu được sử dụng trong mạng tích chập thường bao gồm nhiều kênh, mỗi kênh là một quan sát về một đại lượng khác nhau tại một một điểm nào đó trong không gian hoặc thời gian. Bảng 9.1 trình bày một số ví dụ về các kiểu dữ liệu với số chiều và số lượng kênh khác nhau. | | Đơn kênh | Đa kênh| | ----- |:----------|:----------| | $1$ chiều | Dạng sóng âm thanh: phép tích chập được thực hiên theo trục thời gian. Ta rời rạc hóa thời gian và đo cường độ của sóng tại mỗi bước thời điểm. |Dữ liệu khung xương hoạt hình: Hình ảnh hoạt hình của các nhân vật được kết xuất đồ hoạ (render) $3$ chiều trên máy tính bằng cách thay đổi tư thế của một "khung xương" qua thời gian. Tại mỗi thời điểm, tư thế của nhân vật được biểu diễn bằng một đặc tả về các góc của mỗi khớp trong khung xương nhân vật. Mỗi kênh trong dữ liệu mà ta đưa vào mô hình tích chập đại diện cho góc của một khớp trên một trục toạ độ.| | $2$ chiều |Dữ liệu âm thanh đã được tiền xử lý bằng phép biến đổi Fourier: Ta có thể biến đổi dạng sóng âm thanh thành một tensor $2$ chiều, với các hàng tương ứng với các tần số khác nhau và các cột tương ứng với các thời điểm khác nhau. Sử dụng phép tích chập theo thời gian khiến cho mô hình có tính đẳng biến đối với phép tịnh tiến theo thời gian. Sử dụng phép tích chập dọc theo trục tần số khiến cho mô hình có tính đẳng biến đối với tần số, để cho cùng một giai điệu được chơi trong các quãng tám khác nhau sẽ tạo ra cùng một biểu diễn nhưng ở một độ cao khác nhau trong đầu ra của mạng.| Dữ liệu ảnh màu: Một kênh chứa các điểm ảnh màu đỏ, một kênh chứa các điểm ảnh màu lục và một kênh chứa các điểm ảnh màu lam. Lõi tích chập di chuyển qua cả trục tung lẫn trục hoành của hình ảnh, tạo ra tính đẳng biến đối với tịnh tiến theo cả hai hướng.| | $3$ chiều | Dữ liệu kiểu thể tích: Một nguồn dữ liệu phổ biến dạng này là các hình ảnh y khoa, chẳng hạn như ảnh chụp cắt lớp vi tính (quét CT). | Dữ liệu video màu: Một trục đại diện cho thời gian, một trục đại diện cho chiều cao của khung hình video và một trục đại diện chiều rộng của khung hình video. | > *Bảng 9.1:* Ví dụ về các định dạng dữ liệu khác nhau có thể được sử dụng với các mạng tích chập. Về một ví dụ về cách áp dụng mạng tích chập cho video, bạn có thể tham khảo nghiên cứu của Chen và cộng sự (2010). Cho đến phần này, ta mới chỉ thảo luận về trường hợp mà mọi mẫu trong dữ liệu huấn luyện và dữ liệu kiểm thử có cùng kích thước không gian. Một ưu điểm của các mạng tích chập là chúng còn có thể xử lý các đầu vào có kích thước không gian khác nhau. Những dạng đầu vào kiểu này về cơ bản không thể biểu diễn được bằng các mạng neuron truyền thống dựa trên phép nhân ma trận. Đó cũng là một lý do thuyết phục chúng ta nên dùng mạng tích chập, ngay cả khi chi phí tính toán và quá khớp không phải là vấn đề đáng ngại. Ví dụ, xét một tập ảnh, trong đó mỗi ảnh có chiều rộng và chiều cao khác nhau. Gần như ta không thể tìm ra cách nào để mô hình hoá đầu vào như vậy với một ma trận trọng số kích thước cố định. Phép tích chập giúp giải quyết trường hợp này một cách đơn giản; ta chỉ cần áp dụng lõi chập với số lần khác nhau tùy theo kích thước đầu vào, và kích thước đầu ra của phép tích chập cũng sẽ thay đổi tương ứng. Phép tích chập có thể được coi như phép nhân ma trận; cùng một lõi tích chập có thể tạo ra các kích thước khác nhau của khối ma trận tuần hoàn đôi cho mỗi kích thước đầu vào. Đôi khi, đầu ra cũng như đầu vào của mạng được phép có kích thước thay đổi, ví như trong tác vụ gán nhãn cho mỗi điểm ảnh của đầu vào. Khi đó, ta không cần thực hiện thêm bước thiết kế nào. Trong một số trường hợp khác, khi mạng phải tạo đầu ra có kích thước cố định, chẳng hạn như khi cần gán một nhãn duy nhất cho toàn bộ hình ảnh. Ta bắt buộc phải thực hiện một số bước thiết kế bổ sung, như chèn một tầng gộp có kích thước vùng gộp tỷ lệ thuận với với kích thước của đầu vào, để duy trì một số lượng cố định các đầu ra sau bước gộp. Một vài ví dụ về chiến lược tinh chỉnh này được mô tả trong hình 9.11. Lưu ý rằng, việc sử dụng phép tích chập để xử lý các đầu vào có kích thước thay đổi chỉ có ý nghĩa khi sự thay đổi này đến từ việc chúng chứa số lượng quan sát khác nhau của cùng một đối tượng - chẳng hạn như các bản ghi âm độ dài khác nhau theo thời gian, các quan sát có độ rộng khác nhau trong không gian, v.v. Phép tích chập không có ý nghĩa trong trường hợp sự thay đổi về kích thước ở đầu vào đến từ việc nó có thể tùy ý bao gồm các loại quan sát khác nhau. Ví dụ, nếu chúng tôi đang xử lý các đơn dự tuyển đại học, và các đặc trưng của chúng tôi bao gồm cả điểm quá trình lẫn điểm thi chuẩn hóa kiến thức, nhưng không phải toàn bộ ứng viên đều làm bài kiểm tra chuẩn hoá kiến thức, vì thế sẽ là vô nghĩa khi nhân chập cùng một bộ trọng số cho với các đặc trưng tương ứng với điểm quá trình, cũng như với các đặc trưng tương ứng với các điểm kiểm tra chuẩn hoá kiến thức. # 9.8 Các thuật toán tích chập hiệu quả Mạng tích chập trong các ứng dụng hiện đại thường bao gồm các mạng có thể chứa nhiều hơn một triệu đơn vị. Do đó, các cách cài đặt tối ưu giúp khai thác các nguồn tài nguyên tính toán song song, như sẽ thảo luận ở phần 12.1, là rất cần thiết. Tuy nhiên, trong nhiều trường hợp, ta cũng có thể tăng tốc độ tính toán tích chập bằng cách lựa chọn một thuật toán tích chập phù hợp. Phép tích chập là tương đương với việc chuyển đổi cả đầu vào và lõi sang *miền tần số* (frequency domain) bằng phép biến đổi Fourier, thực hiện phép nhân theo từng điểm của hai tín hiệu, sau đó chuyển đổi ngược lại về miền thời gian bằng phép biến đổi Fourier nghịch đảo. Với một số bài toán có kích thước nhất định, phương pháp này có thể nhanh hơn phương pháp thực thi đơn thuần sử dụng phép tích chập rời rạc. Khi một lõi $d$-chiều có thể được biểu diễn như một tích ngoài (outer product) của $d$ vector, với một vector trên mỗi chiều, lõi này được gọi là khả phân (separable). Khi một lõi là khả phân, việc sử dụng tích chập đơn thuần sẽ không có hiệu quả. Nó tương đương với việc kết hợp $d$ phép tích chập $1$ chiều với từng vector này. Phương pháp kết hợp sẽ nhanh hơn đáng kể so với thực hiện một phép tích chập $d$ chiều với tích ngoài của chúng. Lõi cũng sẽ nhận ít tham số hơn khi biểu diễn bằng các vector. Nếu lõi chứa $w$ phần tử ở mỗi chiều thì phép tích chập đa chiều đơn thuần sẽ cần thời gian chạy và không gian lưu trữ tham số là $O(w^{\,d})$, trong khi phép tích chập khả phân chỉ cần $O(w \times d)$. Tất nhiên, không phải mọi phép tích chập đều có thể biểu diễn theo bằng cách này. > ***ND***: Tích ngoài (outer product) của hai vector $\mathbf{u},\mathbf{v}$ là ma trận $\mathbf{u}\mathbf{v}^T$. Phép toán này không có tính giao hoán. Đi tìm những cách thực hiện phép tích chập hoặc phép tích chập xấp xỉ nhanh hơn mà không ảnh hưởng tới độ chính xác của mô hình là một hướng nghiên cứu sôi nổi hiện nay. Thậm chí các kỹ thuật nâng cao độ hiệu quả của chỉ riêng quá trình lan truyền thuận đa tầng thôi cũng đã mang lại nhiều lợi ích, bởi vì trong thương mại, đa số các tài nguyên tính toán được sử dụng để phục vụ việc triển khai mô hình thay vì huấn luyện mô hình. # 9.9 Các đặc trưng ngẫu nhiên hoặc không giám sát Thông thường, phần tốn kém chi phí tính toán nhất trong việc huấn luyện mạng neuron tích chập là học các đặc trưng. Tầng đầu ra thường không tốn kém, bởi số lượng đặc trưng đầu vào của tầng này sau khi trải qua nhiều tầng gộp là khá nhỏ. Khi thực hiện huấn luyện có giám sát với trượt gradient, mọi bước trượt gradient đều yêu cầu hoàn thành quá trình tính toán lan truyền thuận và lan truyền ngược qua toàn bộ mạng. Một cách để giảm thiểu chi phí trong huấn luyện mạng tích chập là sử dụng các đặc trưng không được huấn luyện theo kiểu có giám sát. Có ba chiến lược cơ bản để thu đuọc các lõi tích chập mà không cần sử dụng học có giám sát. Một trong số đó là khởi tạo ngẫu nhiên. Một cách khác là thiết kế thủ công, chẳng hạn, thiết lập từng lõi để xác định các cạnh biên ở một hướng hoặc tỉ lệ cụ thể. Cuối cùng, ta có thể học các lõi bằng một tiêu chuẩn không giám sát. Ví dụ, [Coates et al., 2011] áp dụng phân cụm $k$-means đối với các *mảnh* (patch) hình ảnh nhỏ , sau đó sử dụng từng trọng tâm (centroid) sau khi học được làm lõi chập. Trong Phần III, ta sẽ mô tả nhiều hướng tiếp cận dựa trên học không giám sát hơn. Học các đặc trưng với một tiêu chuẩn không giám sát cho phép các đặc trưng này được xác định một cách tách biệt với tầng phân lớp ở đỉnh của mô hình. Sau đó, ta có thể trích xuất các đặc trưng cho toàn bộ tập huấn luyện chỉ trong một lần, về cơ bản xây dựng một tập huấn luyện mới cho tầng cuối cùng. Quá trình học tầng cuối cùng sau đó chỉ là một bài toán tối ưu lồi, giả định rằng tầng cuối cùng là hồi quy logit hoặc máy vector hỗ trợ (SVM). Các bộ lọc ngẫu nhiên thường cho kết quả tốt đáng ngạc nhiên đối với mạng tích chập [Jarrett et al., 2009; Saxe et al., 2011; Pinto et al., 2011; Cox and Pinto, 2011]. Nghiên cứu của Saxe và cộng sự (2011) cho thấy các tầng bao gồm phép tích chập theo sau bởi một tầng gộp, một cách tự nhiên, sẽ có tính chọn lọc tần số, và trở nên bất biến với phép tịnh tiến khi được gán các trọng số ngẫu nhiên. Họ lập luận rằng điều này cung cấp cho ta một cách ít tốn kém để lựa chọn kiến trúc của mạng tích chập: đầu tiên, đánh giá hiệu suất của một số kiến trúc mạng tích chập bằng cách chỉ huấn luyện tầng cuối cùng, sau đó chọn ra kiến trúc tốt nhất trong số này và huấn luyện toàn bộ kiến trúc đó bằng một hướng tiếp cận có tốn kém hơn. Một hướng tiếp cận trung gian là học các đặc trưng, nhưng bằng cách sử dụng các phương thức không yêu cầu lan truyền thuận và lan truyền ngược đầy đủ tại mỗi bước trượt gradient. Với những mạng perceptron đa tầng, ta sử dụng tiền huấn luyện tham lam theo từng tầng, huấn luyện tầng đầu tiên một cách cô lập, sau đó trích xuất toàn bộ đặc trưng từ tầng đầu tiên một lần duy nhất, tiếp theo, huấn luyện tầng thứ hai một cách cô lập với những đặc trưng đó, và cứ tiếp tục như thế. Trong chương 8, ta đã tìm hiểu cách thực hiện tiền huấn luyện tham lam có giám sát theo từng tầng, và phần III sẽ mở rộng quy trình này bằng cách sử dụng tiêu chuẩn không giám sát tại mỗi tầng. Một ví dụ kinh điểm của tiền huấn luyện tham lam theo từng tầng của một mô hình tích chập là mạng phân phối đa tầng tích chập (convolutional deep belief network) [Lee et al., 2009]. Các mạng tích chập cho chúng ta cơ hội để đưa chiến lược tiền huấn luyện đi xa hơn một bước so với những gì mà perceptron đa tầng có thể làm. Thay vì huấn luyện toàn bộ tầng tích chập tại một thời điểm, ta có thể huấn luyện một mô hình theo từng mảnh nhỏ, giống như Coates và cộng sự (2011) đã làm với $k$-means. Sau đó ta có thể sử dụng các tham số từ mô hình theo mảnh nhỏ này để xác định các lõi của tầng tích chập. Điều này có nghĩa là ta có thể sử dụng học không giám sát để huấn luyện mạng tích chập mà *không cần sử dụng phép tích chập trong quá trình huấn luyện*. Với cách tiếp cận này, ta có thể huấn luyện các mô hình lớn và tiêu tốn tài nguyên tính toán chỉ tại bước suy luận (inference) [Ranzato et al., 2007b; Jarrett et al.,2009; Kavukcuoglu et al., 2010; Coates et al., 2013]. Cách tiếp cận này trở nên phổ biến từ khoảng năm 2007 đến năm 2013, khi các tập dữ liệu đã gán nhãn còn nhỏ và sức mạnh tính toán còn giới hạn. Ngày nay, hầu hết các mạng tích chập đã được huấn luyện theo cách hoàn toàn có giám sát, sử dụng lan truyền thuận và ngược qua toàn bộ mạng theo từng bước lặp huấn luyện. Giống như các hướng tiếp cận khác cho bài toán tiền huấn luyện không giám sát, rất khó để tách bạch nguyên nhân của những ích lợi mà phương pháp này mang lại. Tiền huấn luyện không giám sát có thể cho ta một cơ chế kiểm soát nào đó liên quan đến huấn luyện có giám sát, hoặc đơn giản chỉ là cho phép ta huấn luyện các kiến trúc lớn hơn nhiều bởi vì nó giúp giảm chi phí tính toán của quy tắc học. # 9.10 Nền tảng thần kinh học của mạng tích chập Mạng tích chập có lẽ là câu chuyện thành công rực rỡ nhất về trí thông minh nhân tạo lấy cảm hứng từ sinh học. Mặc dù mạng tích chập cũng được dẫn lối bởi nhiều lĩnh vực khác, nhưng các nguyên tắc thiết kế chủ chốt của mạng được rút ra từ chính khoa học thần kinh. Lịch sử của mạng tích chập bắt đầu với các thí nghiệm khoa học thần kinh khá lâu trước khi các mô hình tính toán liên quan được phát triển. Hai nhà thần kinh học David Hubel và Torsten Wiesel đã cộng tác trong nhiều năm trời để xác định được các cơ sở cơ bản nhất về cách mà hệ thống thị giác của loài động vật có vú hoạt động [Hubel and Wiesel, 1959, 1962, 1968]. Thành tựu của họ cuối cùng đã được công nhận bằng giải Nobel. Khám phá của họ đã mang đến nguồn cảm hứng to lớn cho các mô hình học sâu đương thời dựa trên việc ghi lại hoạt động của các neuron riêng lẻ ở mèo. Họ quan sát cách neuron trong não mèo phản ứng với những hình ảnh chiếu ở những vị trí chính xác trên màn hình đặt trước mặt nó. Và họ phát hiện ra rằng các neuron trong hệ thống thị giác ban đầu phản ứng mạnh mẽ nhất với các mô thức ánh sáng cụ thể, chẳng hạn như các vạch định hướng, nhưng hầu như không phản ứng rõ ràng với các mô thức khác. Thành quả của họ đã giúp mô tả nhiều khía cạnh của chức năng não, những thứ vượt quá phạm vi của cuốn sách này. Từ quan điểm của học sâu, ta có thể tập trung vào một góc nhìn đơn giản, minh hoạ về chức năng của não. Dưới góc nhìn đơn giản này, ta tập trung vào phần não bộ có tên gọi là $V1$, còn được gọi là *vỏ não thị giác sơ cấp* (primary visual cortex). $V1$ là vùng đầu tiên của bộ não bắt đầu thực hiện một cách rõ ràng việc xử lý cấp cao các tín hiệu thị giác truyền vào. Trong góc nhìn này, hình ảnh được hình thành bởi ánh sáng đến mắt và kích thích võng mạc - một mô nhạy cảm với ánh sáng, nằm ở phía sau của mắt. Các neuron trong võng mạc thực hiện một số tiền xử lý hình ảnh đơn giản, nhưng không thay đổi đáng kể nó được biểu diễn. Hình ảnh sau đó đi qua dây thần kinh thị giác và một vùng não được gọi là *nhân di truyền biên* (lateral geniculate nucleus - $LGN$). Vai trò chính của cả hai vùng giải phẫu này, theo những gì chúng ta đang quan tâm, chủ yếu chỉ là mang tín hiệu từ mắt đến $V1$ nằm ở phía sau đầu. Một tầng mạng tích chập được thiết kế để nắm bắt ba thuộc tính của $V1$: 1. $V1$ được sắp xếp trong một ánh xạ không gian. Nó thực chất có một cấu trúc $2$ chiều, phản chiếu cấu trúc của hình ảnh trong võng mạc. Ví dụ, ánh sáng chiếu tới nửa dưới của võng mạc chỉ ảnh hưởng đến nửa tương ứng của $V1$. Các mạng tích chập nắm bắt thuộc tính này bằng các đặc trưng được định nghĩa theo dạng ánh xạ $2$ chiều. 2. $V1$ chứa nhiều *tế bào đơn giản*. Một hoạt động của tế bào đơn giản này ở một mức độ nào đó có thể được đặc trưng bởi một hàm tuyến tính của hình ảnh trong một trường tiếp nhận nhỏ và được cục bộ hoá theo không gian. Các đơn vị phát hiện của một mạng tích chập được thiết kế để mô phỏng các đặc tính này của các tế bào đơn giản. 3. $V1$ cũng chứa nhiều tế bào phức tạp. Những tế bào này phản ứng với các đặc trưng tương tự các đặc trưng được phát hiện bởi các tế bào đơn giản, nhưng các tế bào phức tạp lại bất biến đối với các dịch chuyển nhỏ về vị trí của đặc trưng. Điều này truyền cảm hứng cho các đơn vị gộp của mạng tích chập. Các tế bào phức tạp cũng bất biến đối với một số thay đổi về ánh sáng vốn không thể được nắm bắt chỉ bằng cách gộp các vị trí không gian. Sự bất biến này đã truyền cảm hứng cho một số chiến lược gộp kênh chéo trong các mạng tích chập, chẳng hạn như các đơn vị đầu ra cực đại [Goodfellow et al., 2013a]. Mặc dù gần như chỉ biết về $V1$, nhưng người ta tin rằng các nguyên tắc cơ bản y hệt cũng được áp dụng vào các vùng khác của hệ thống thị giác. Trong góc nhìn đơn giản phía trên của ta về hệ thống thị giác, chiến lược cơ bản của việc phát hiện theo sau bởi phép gộp được áp dụng lặp đi lặp lại khi ta tiến sâu hơn vào não. Khi đi qua nhiều tầng kết cấu của não, cuối cùng chúng ta sẽ tìm ra được các tế bào phản ứng với một số khái niệm cụ thể và bất biến đối với nhiều phép biến đổi đầu vào. Những tế bào này được đặt biệt danh là “tế bào bà ngoại” (grandmother cells) - dựa theo ý tưởng rằng một người có thể có một neuron được kích hoạt khi nhìn thấy hình ảnh của bà ngoại, bất kể bà xuất hiện ở bên trái hay bên phải bức ảnh, chụp cận cảnh khuôn mặt hay phóng rộng ra toàn bộ người, dù được chiếu sáng hay ở trong bóng tối, v.v. Những tế bào này đã được chứng minh là thực sự tồn tại trong não người, trong một vùng gọi là *thùy thái dương trung tâm* [Quiroga et al., 2005]. Các nhà nghiên cứu đã kiểm tra xem liệu các neuron riêng lẻ có phản ứng với ảnh của những người nổi tiếng hay không. Họ đã tìm thấy thứ được gọi là “neuron Halle Berry”, tức một neuron riêng lẻ được kích hoạt theo khái niệm của Halle Berry. Tế bào thần kinh này kích hoạt khi một người nhìn thấy ảnh của Halle Berry, một bản vẽ Halle Berry, hoặc thậm chí văn bản có chứa cụm từ "Halle Berry." Tất nhiên, điều này không liên quan gì đến bản thân Halle Berry; cũng có các neuron khác đáp lại sự hiện diện của Bill Clinton, Jennifer Aniston, v.v. Các neuron thùy thái dương trung tâm này có tính tổng quát cao hơn các mô hình mạng tích chập hiện đại, vốn sẽ không tự động khái quát hóa để xác định một người hoặc đối tượng khi đọc được tên của họ. Thứ tương tự nhất với tầng cuối cùng của một mạng tích chập các đặc trưng là một vùng não được gọi là *võ não phía dưới của thùy thái dương* (inferotemporal cortex) ($IT$). Khi nhìn thấy một vật thể, thông tin từ võng mạc, thông qua $LGN$, đến $V1$, sau đó chuyển sang $V2$, sau đó là $V4$, $IT$. Điều này xảy ra trong 100 mili giây đầu tiên khi ta nhìn thoáng qua vật thể. Nếu được phép tiếp tục nhìn vào vật thể trong thời gian dài hơn, thì thông tin sẽ bắt đầu truyền ngược vì não sử dụng phản hồi từ trên xuống để cập nhật các kích hoạt ở các vùng não cấp thấp hơn. Tuy nhiên, nếu ta làm gián đoạn tầm nhìn của người đó và chỉ quan sát các tỷ lệ kích phát xảy ra do sự kích hoạt lan truyền thuận trong 100 mili giây đầu tiên gây ra, thì $IT$ sẽ giống với một mạng tích chập. Các mạng tích chập có thể dự đoán tỷ lệ kích phát $IT$ và thực hiện tương tự như con người (có giới hạn thời gian) đối với các tác vụ nhận dạng vật thể [DiCarlo, 2013]. Điều đó nói lên rằng, có rất nhiều sự khác biệt giữa các mạng tích chập và hệ thống thị giác của động vật có vú. Một vài trong số những khác biệt này được các nhà thần kinh học tính toán hiểu rất rõ nhưng nằm ngoài phạm vi cuốn sách này. Một vài điểm khác biệt khác thì chưa được biết đến, bởi nhiều câu hỏi cơ bản về cách hệ thống thị giác của loài động vật có vú hoạt động vẫn còn là ẩn số. Có thể tóm lược như sau: - Mắt người hầu như có độ phân giải rất thấp, ngoại trừ một mảnh nhỏ gọi là *hố thị giác* (fovea). Hố thị giác chỉ quan sát một khu vực có kích thước bằng độ dài móng tay cái. Mặc dù ta cảm thấy mình có thể nhìn thấy toàn bộ khung cảnh ở độ phân giải cao, nhưng thực ra đó chỉ là một ảo tưởng được tạo ra bởi phần tiềm thức của bộ não, khi nó chắp vá những cái nhìn lướt qua của các khu vực nhỏ. Hầu hết các mạng tích chập đều thực sự nhận được một lướng lớn hình ảnh có độ phân giải cao làm đầu vào. Bộ não con người thực hiện một số chuyển động mắt gọi là *liếc nhanh* (saccade) để nhìn thoáng qua những phần nổi bật hoặc liên quan nhất của khung cảnh. Kết hợp các cơ chế chú ý (attention mechanism) tương tự như vậy vào các mô hình học tập sâu là một hướng nghiên cứu đang được quan tâm. Trong học sâu, các cơ chế chú ý đã được áp dụng thành công nhất trong lĩnh vực xử lý ngôn ngữ tự nhiên, như sẽ được mô tả trong phần 12.4.5.1. Một số mô hình thị giác với các cơ chế hố thị giác đã được phát triển nhưng cho đến nay vẫn chưa trở thành hướng tiếp cận chủ yếu [Larochelle and Hinton, 2010; Denil et al., 2012]. - Hệ thống thị giác của con người được tích hợp với nhiều giác quan khác, như thính giác, và các yếu tố khác như tâm trạng và suy nghĩ. Mạng tích chập cho đến nay chỉ hoàn toàn là thị giác. - Hệ thống thị giác của con người thực hiện nhiều công việc hơn chỉ đơn giản là nhận biết các vật thể. Nó có thể hiểu được toàn cảnh, bao gồm nhiều vật thể và mối quan hệ giữa các vật thể, và nó xử lý tốt thông tin hình học $3$ chiều cần thiết cho cơ thể để giao tiếp với vạn vật. Mạng tích chập đã được áp dụng vào một vài trong số những vấn đề này nhưng các ứng dụng vẫn chỉ đang ở giai đoạn sơ khai. - Ngay cả những vùng não đơn giản như $V1$ cũng bị ảnh hưởng mạnh mẽ bởi phản hồi từ các vùng bậc cao. Phản hồi đã được khảo sát rộng rãi trong các mô hình mạng neuron nhưng chưa được chứng minh là đưa ra một cải tiến thuyết phục nào. - Khi IT lan truyền thuận kích phát nắm bắt được lượng thông tin tương tự như các đặc trưng trong mạng tích chập, ta không biết rõ rằng các tính toán trung gian là giống nhau như thế nào. Não có thể sử dụng các hàm kích hoạt và phép gộp rất khác. Có lẽ biểu thị hàm kích hoạt của một neuron riêng lẻ bằng một phản hồi tuyến tính đơn lẻ không hẳn là đã tốt. Một mô hình gần đây của $V1$ chứa nhiều bộ lọc bậc $2$ cho mỗi neuron (Rust et al., 2005). Thực ra, hình ảnh minh hoạ của ta về các “tế bào đơn giản” và “tế bào phức tạp” có thể tạo ra một sự phân biệt không có thật; các tế bào đơn giản và phức tạp có thể là cùng một loại tế bào, nhưng các "tham số" của chúng kích hoạt một dải hành vi liên tục trải rộng từ mức ta gọi là "đơn giản" đến mức mà ta gọi là "phức tạp". Một điều đáng nói là khoa học thần kinh chỉ cho chúng ta tương đối ít thông tin về cách làm thế nào để huấn luyện mạng tích chập. Các kiến trúc mô hình dùng chung tham số qua nhiều vị trí không gian được khởi nguồn từ các mô hình kết nối thị giác ban đầu [Marr and Poggio, 1976], nhưng các mô hình này không sử dụng thuật toán lan truyền ngược và trượt gradient hiện đại. Ví dụ, neocognitron [Fukushima, 1980] kết hợp hầu hết các yếu tố thiết kế kiến trúc mô hình của mạng tích chập hiện đại nhưng dựa trên một thuật toán phân cụm không giám sát theo từng tầng. Lang và Hinton (1988) đã giới thiệu việc sử dụng lan truyền ngược để huấn luyện *mạng neuron có độ trễ* (time-delay neural network - TDNN). Theo thuật ngữ đương đại, mạng neuron có độ trễ là mạng tích chập $1$ chiều được áp dụng cho chuỗi thời gian. Quá trình lan truyền ngược áp dụng trong các mô hình này không lấy cảm hứng từ bất kỳ quan sát thần kinh nào và được một số người coi là không hợp lý về mặt sinh học. Tiếp nối thành công của việc huấn luyện dựa trên lan truyền ngược của mạng neuron có độ trễ, [LeCun et al., 1989] đã phát triển mạng tích chập hiện đại bằng cách áp dụng cùng một thuật toán huấn luyện cho phép tích chập $2$ chiều được áp dụng cho hình ảnh. Đến nay, ta đã hiểu lí do các tế bào đơn giản gần như là tuyến tính và có tính chọn lọc đối với các đặc trưng nhất định, còn các tế bào phức tạp lại có tính phi tuyến hơn và trở nên bất biến đối với một số phép biến đổi các đặc trưng của tế bào đơn giản. Ngăn xếp của các tầng nằm xen kẽ giữa tính chọn lọc và tính bất biến có thể sinh ra các tế bào "bà ngoại" cho những hiện tượng cụ thể. Chúng tôi vẫn chưa mô tả chính xác những gì mà các tế bào riêng lẻ này phát hiện được. Trong một mạng phi tuyến tính đa tầng, khó có thể hiểu được chức năng của các tế bào riêng biệt. Các tế bào đơn giản trong tầng đầu tiên sẽ là dễ phân tích hơn, vì phản hồi của chúng được điều khiển bởi hàm tuyến tính. Trong một mạng neuron nhân tạo, ta chỉ có thể hiển thị hình ảnh của lõi chập để xem các kênh tương ứng nào của mạng tích chập sẽ phản hồi. Trong một mạng neuron sinh học, ta không thể tiếp cận được các trọng số của chính nó. Thay vào đó, ta đặt một điện cực vào neuron, hiển thị một số mẫu hình ảnh nhiễu trắng ở phía trước võng mạc của động vật và ghi lại cách mỗi mẫu này kích hoạt neuron. Sau đó, ta có thể khớp một mô hình tuyến tính với những phản hồi này để đạt được một xấp xỉ cho các trọng số của neuron. Cách tiếp cận này được gọi là *tương quan nghịch đảo* (reverse correlation) [Ringach and Shapley, 2004]. Tương quan nghịch đảo cho chúng ta thấy rằng: hầu hết các tế bào $V1$ có trọng số được mô tả bởi các *hàm Gabor*. Hàm Gabor mô tả trọng số tại một điểm ảnh $2$ chiều trong bức ảnh. Ta có thể coi một bức ảnh như là một hàm của các tọa độ $2$ chiều, $I (x, y)$. Tương tự như vậy, có thể coi một tế bào đơn giản như phép lấy mẫu hình ảnh tại một tập hợp các tọa độ, được định nghĩa bởi tập hợp $\mathbb{X}$ của các tọa độ $x$ và tập hợp $\mathbb{Y}$ của các tọa độ $y$, sau đó áp dụng các trọng số mà cũng là một hàm của vị trí, $w (x, y)$. Từ góc nhìn này, phản ứng của một tế bào đơn giản đối với một hình ảnh được cho bởi $$\begin{eqnarray} s(I) = \sum_{x \in \mathbb{X}} \sum_{y \in \mathbb{Y}}w(x,y)I(x,y), \tag{9.15} \end{eqnarray}$$ Cụ thể, $w(x, y)$ có dạng hàm Gabor: $$\begin{eqnarray} w(x,y;\alpha,\beta_x,\beta_y,f,\phi,x_0,y_0,\tau) = \alpha \exp(-\beta_x x'^{2} -\beta_y y'^{2}) \cos(fx'+\phi) \tag{9.16} \end{eqnarray}$$ trong đó $$\begin{eqnarray} x' = (x - x_0)\cos(\tau) + (y-y_0)\sin(\tau) \tag{9.17} \end{eqnarray}$$ và $$\begin{eqnarray} y' = -(x-x_0)\sin(\tau) + (y-y_0)\cos(\tau) \tag{9.18} \end{eqnarray}$$ Ở đây $\alpha , \beta_x , \beta_y , f , \phi , x_0 , y_0$ và $\tau$ là các tham số kiểm soát các thuộc tính của hàm Gabor. Hình 9.18 trình bày một số ví dụ về các hàm Gabor với các thiệt lập khác nhau cho những tham số này. ![](https://i.imgur.com/fEQR3Yq.png) > Hình 9.18: Hàm Gabor với nhiều thiết lập tham số khác nhau. Màu trắng biểu thị trọng số dương lớn, màu đen biểu thị trọng số âm lớn và nền màu xám tương ứng với trọng số bằng $0$. *(Hình trái)* Các hàm Gabor với các giá trị khác nhau của các tham số điều khiển hệ tọa độ: $x_0$, $y_0$ và $\tau$. Mỗi hàm Gabor trong lưới này được gán giá trị $x_0$ và $y_0$ tỉ lệ với vị trí của nó trong lưới, và $\tau$ được chọn sao cho mỗi bộ lọc Gabor sẽ nhạy với hướng toả ra từ trung tâm của lưới. Đối với hai hình còn lại, $x_0$, $y_0$ và $\tau$ được gán cố định bằng $0$. *(Hình giữa)* Hàm Gabor với các tham số tỉ lệ Gauss khác nhau $β_x$ và $β_y$. Các hàm Gabor được sắp xếp theo sự tăng chiều rộng (giảm $β_x$) khi di chuyển từ trái qua phải, và sự tăng chiều cao (giảm $β_y$) khi di chuyển từ trên xuống dưới. Đối với hai hình còn lại, giá trị $\beta$ được cố định bằng $1.5$ lần chiều rộng của ảnh. *(Hình phải)* Hàm Gabor với các tham số dạng hình $sin$ khác nhau $f$ và $\phi$. Khi di chuyển từ trên xuống dưới, $f$ tăng lên, và khi dịch chuyển từ trái sang phải, $\phi$ tăng lên. Đối với hai hình còn lại, $\theta$ được gán bằng $0$ và $f$ được được cố định bằng $5$ lần chiều rộng của ảnh. Các tham số $x_0$, $y_0$ và $\tau$ giúp xác định một hệ tọa độ. Ta tịnh tiến và xoay $x$ và $y$ để tạo thành $x'$ và $y'$. Cụ thể, tế bào đơn giản sẽ phản hồi các đặc trưng của ảnh tập trung tại điểm ($x_0$, $y_0$), và nó sẽ phản ứng với những thay đổi về độ sáng khi ta di chuyển dọc theo một đường thẳng được xoay đi một góc $\tau$ radian so với phương ngang. Được xem như một hàm số của $x'$ và $y'$, $w$ sau đó phản ứng với những thay đổi về độ sáng khi ta di chuyển dọc theo trục $x'$. Nó có hai yếu tố quan trọng: một là hàm Gauss, hai là hàm $\cos$. Nhân tử Gauss $\alpha\exp(− β_xx'^2− β_yy'^2)$ có thể xem như là một *số hạng chặn cửa* (gating term) để đảm bảo rằng tế bào đơn giản sẽ chỉ phản ứng với các giá trị gần khi $x'$ và $y'$ đều bằng $0$, hay nói cách khác là gần với trung tâm trường tiếp nhận của tế bào. Nhân tử tỉ lệ $\alpha$ điều chỉnh tổng độ lớn của các phản ứng từ tế bào đơn giản, trong khi $β_x$ và $β_y$ kiểm soát tốc độ trường tiếp nhận của nó bị thu hẹp. Nhân tử $cos(fx'+ \phi)$ điều khiển cách một tế bào đơn phản ứng với sự thay đổi độ sáng dọc theo trục $x'$. Tham số $f$ điều khiển tần số của hàm $\cos$, và $\phi$ điều khiển độ lệch pha của nó. Nhìn chung, góc nhìn minh hoạ về các tế bào đơn giản có nghĩa là một tế bào đơn giản phản ứng với tần số không gian cụ thể của độ sáng theo một hướng cụ thể tại một vị trí cụ thể. Các tế bào đơn giản bị kích thích nhất khi sóng của ánh sáng trong ảnh có cùng pha với các trọng số. Điều này xảy ra khi bức ảnh là sáng ở những nơi có trọng số dương và tối ở những nơi có trọng số âm. Các tế bào đơn giản gần như bị vô hiệu khi sóng của ánh sáng hoàn toàn lệch pha với các trọng số — khi hình ảnh là tối ở nơi có các trọng số dương và sáng ở nơi có các trọng âm. Góc nhìn hoạt hình của một tế bào phức tạp đó là: nó tính toán chuẩn $L^2$ của vector $2$ chiều chứa hai phản hồi của tế bào đơn giản: $c(I) =\sqrt{s_0(I)^2+ s_1(I)^2}$. Một trường hợp đặc biệt quan trọng xảy ra khi $s_1$ có cùng những tham số giống như $s_0$ ngoại trừ $\phi$, và $\phi$ được thiết lập sao cho $s_1$ lệch pha $1/4$ chu kỳ so với $s_0$. Trong trường hợp này, $s_0$ và $s_1$ tạo thành một *cặp cầu phương* (vuông góc). Một tế bào phức tạp được xác định theo cách này sẽ phản ứng khi hình ảnh kiểu Gauss được đánh trọng số lại $I (x, y) \exp (−\beta_xx^2 − \beta_yy^2)$ chứa một sóng $sin$ có biên độ cao với tần số $f$ theo hướng $\tau$ gần $(x_0, y_0)$, *bất chấp độ lệch pha ban đầu của sóng này*. Nói cách khác, tế bào phức tạp bất biến đối với các tịnh tiến nhỏ của hình ảnh theo hướng $\tau$, hoặc đối với phép đổi dấu hình ảnh (thay thế màu đen bằng màu trắng và ngược lại). Sự liên đới nổi bật nhất giữa khoa học thần kinh và học máy xuất phát từ việc so sánh một cách trực quan các đặc trưng học được bởi các mô hình học máy với những đặc trưng được sử dụng bởi $V1$. Olshausen và Field (1996) đã chỉ ra rằng một thuật toán học không giám sát đơn giản, mã hóa thưa, học các đặc trưng với các trường tiếp nhận tương tự như các tế bào đơn giản. Kể từ đó, chúng ta đã tìm ra một loạt các thuật toán học thống kê đa dạng giúp học các đặc trưng với các hàm dạng Gabor khi áp dụng cho các hình ảnh tự nhiên. Hầu hết các thuật toán học sâu cũng có tính chất này và chúng học các đặc trưng đó ở tầng đầu tiên. Hình 9.19 sẽ cho ta thấy một vài ví dụ. Do có rất nhiều thuật toán học tập khác nhau dùng để học bộ phát hiện cạnh biên, rất khó để kết luận rằng một thuật toán học cụ thể nào đó là mô hình “đúng đắn” của não nếu chỉ dựa trên các đặc trưng mà nó học được (dù nó có thể chắc chắn là một dấu hiệu không tốt nếu thuật toán *không* học được một số bộ phát hiện cạnh biên khi áp dụng cho các hình ảnh tự nhiên). Những đặc trưng này là một phần quan trọng trong cấu trúc thống kê của các hình ảnh tự nhiên, và có thể được phục hồi bằng nhiều hướng tiếp cận khác nhau với mô hình hoá thống kê. Tham khảo nghiên cứu của Hyvärinen và cộng sự (2009) để lượt qua mảng thống kê hình ảnh tự nhiên. ![](https://i.imgur.com/HpQBuTG.png) > Hình 9.19: Nhiều thuật toán học máy học các đặc trưng giúp phát hiện các cạnh biên, hoặc màu sắc cụ thể của các cạnh biên khi áp dụng cho hình ảnh tự nhiên. Các bộ phát hiện đặc trưng này gợi nhớ đến các hàm Gabor có trong võ não thị giác sơ cấp. *(Hình trái)* Trọng số học được bởi một thuật toán học không giám sát (mã hóa thưa nhọn-và-bằng) áp dụng cho các mảnh hình ảnh nhỏ. *(Hình phải)* Các lõi chập học được bởi tầng đầu tiên của một mạng tích chập cực đại đầu ra có giám sát đầy đủ. Các cặp bộ lọc kề nhau ảnh hưởng đến cùng một đơn vị đầu ra cực đại. # 9.11 Mạng tích chập và lịch sử của học sâu Mạng tích chập đã đóng một vai trò quan trọng trong lịch sử phát triển của học sâu. Chúng là một ví dụ điển hình cho việc ứng dụng thành công những hiểu biết từ nghiên cứu não bộ rồi áp dụng cho các ứng dụng học máy. Cũng có một số mô hình học sâu đầu tiên mang lại kết quả tốt, khá lâu trước khi các mô hình học sâu được xem là mang tính khả thi để sử dụng trong thực tế. Mạng tích chập cũng là một trong số những mạng neuron đầu tiên giải quyết các ứng dụng thương mại quan trọng và vẫn đang đi đầu trong các ứng dụng thương mại về học sâu hiện nay. Ví dụ, trong thập niên 1990, nhóm nghiên cứu neuron tại AT&T đã phát triển một mạng tích chập để đọc các tấm séc [LeCun et al., 1998b]. Vào cuối những năm 1990, hệ thống này được tập đoàn NRC triển khai và đã đọc trên $10\%$ tổng số lượng séc ở Hoa Kỳ. Sau đó, một số hệ thống nhân dạng ký tự quang học (OCR) và nhận dạng chữ viết tay dựa trên mạng tích chập đã được Microsoft triển khai [Simard et al., 2003]. Xem chương 12 để biết thêm chi tiết về các ứng dụng như vậy cũng như các ứng dụng hiện đại hơn nữa của mạng tích chập. Tham khảo nghiên cứu của Lecun và cộng sự (2010) để hiểu cặn kẽ hơn về lịch sử của mạng tích chập đến năm 2010. Mạng tích chập cũng được sử dụng để giành chiến thắng trong nhiều cuộc thi. Sự thương mại hoá học sâu bắt đầu trở nên mạnh mẽ kể từ khi Krizhevsky và cộng sự (2012) chiến thắng cuộc thi nhận dạng vật thể ImageNet, mặc dù mạng tích chập đã được sử dụng và giành chiến thắng trong các cuộc thi học máy và thị giác máy tính khác từ các năm trước. Mạng tích chập là một trong số những mạng đa tầng đầu tiên được huấn luyện với thuật toán lan truyền ngược. Không có giải thích hoàn toàn rõ ràng nào về lí do các mạng tích chập lại thành công trong khi các mạng lan truyền ngược tổng quát đã bị xem là thất bại. Có thể đơn giản là vì các mạng tích chập hiệu quả về mặt tính toán hơn các mạng liên kết đầy đủ, do đó nó dễ dàng chạy nhiều thử nghiệm, cũng như dễ dàng trong việc tinh chỉnh cách triển khai và tinh chỉnh các siêu tham số hơn. Các mạng lớn hơn cũng có vẻ dễ huấn luyện hơn. Với phần cứng hiện đại, các mạng liên kết đầy đủ cỡ lớn có vẻ như sẽ thực hiện khá tốt nhiều tác vụ, ngay cả khi sử dụng các tập dữ liệu có sẵn và các hàm kích hoạt phổ biến trong suốt khoảng thời gian mà mạng liên kết đầy đủ bị cho là không hoạt động tốt. Có thể rào cản chính đối với sự thành công của mạng neuron chính là tâm lý (người ta không mong đợi rằng mạng neuron hoạt động được, nên họ đã không đầu tư đủ nghiêm túc để sử dụng được mạng neuron). Dù thế nào đi chăng nữa, thật may mắn là các mạng tích chập đã hoạt động tốt từ nhiều thập kỷ trước. Bằng nhiều cách, chúng đã thắp lên niềm hy vọng cho toàn bộ phần còn lại của học sâu và mở đường cho kỷ nguyên của mạng neuron nói chung. Mạng tích chập cho ta một cách thức chuyên biệt hóa các mạng neuron để trở nên hữu hiệu với dữ liệu có cấu trúc liên kết dạng lưới rõ ràng, và mở rộng các mô hình như vậy lên quy mô cực lớn. Phương pháp này đã trở thành hướng tiếp cận thành công nhất đối với cấu trúc liên kết dạng hình ảnh $2$ chiều. Để xử lý dữ liệu tuần tự $1$ chiều, ta sẽ chuyển tới một dạng mạng neuron chuyên biệt mạnh mẽ khác: mạng neuron truy hồi. ------------------- ***Việt hóa hình ảnh:*** Phạm Hoàng Nhật <br> ***Người dịch:*** Mai Anh, Sơn Hoàng Trần, Thomas Nguyễn, Dương Nguyễn, Hùng Lê, Võ Tấn Phát, Nguyễn Ánh Dương, Trần Duy Thanh, Nam Trần, Xuân Tú, Thu.Nguyen, Vân, lhlong
